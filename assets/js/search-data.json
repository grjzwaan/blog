{
  
    
        "post0": {
            "title": "Fit polynomials 3",
            "content": "Introduction . This is the third part of &#39;Fit with polynomials&#39;. Check out the first part and the second part. . In this post we&#39;ll look at the oscillating behaviour. You can see that fitting the Runge function with more sampling points (and higher degree) becomes a better fit in the middle, but oscillates at the ends. This is known as the Runge&#39;s phenomenon. . d = np.linspace(-1, 1, 101) . . def interpolate(f, x): M = np.vander(x, increasing=True) c = np.linalg.inv(M) @ f(x) return c . . fig, axs = plot.subplots(2, 1) fig.set_size_inches(18.5, 10.5) x1 = np.linspace(-1, 1, 5) runge = lambda x: 1/(1+(5*x)**2) pf = np.polynomial.Polynomial(interpolate(runge, x1)) axs[0].plot(x1, runge(x1), &#39;o&#39;) axs[0].plot(d, runge(d), &#39;-&#39;) axs[0].plot(d, pf(d), &#39;-&#39;) axs[0].set_title(&#39;Runge function, 5 points&#39;) x2 = np.linspace(-1, 1, 10) pg = np.polynomial.Polynomial(interpolate(runge, x2)) axs[1].plot(x2, runge(x2), &#39;o&#39;) axs[1].plot(d, runge(d), &#39;-&#39;) axs[1].plot(d, pg(d), &#39;-&#39;) axs[1].set_title(&#39;Runge function, 10 points&#39;) for ax in axs.flat: ax.label_outer() . . Non-regular sampling: the intuition . The idea that stuck with me was to imagine that the interpolation has a lot of freedom outside $[-1, 1]$. So by fixating points on regular intervals the points in the middle are controlled much more than at the edges. Given that we have to pick points within $[-1,1]$ we should sample more densily at the edges. . For example, we can pick $[-1, -0.8, -0.5, 0, 0.5, 0.8, 1]$ and compare to the equidistant case: . fig, axs = plot.subplots(2, 1) fig.set_size_inches(18.5, 10.5) x1 = np.linspace(-1, 1, 7) runge = lambda x: 1/(1+(5*x)**2) pf = np.polynomial.Polynomial(interpolate(runge, x1)) axs[0].plot(x1, runge(x1), &#39;o&#39;) axs[0].plot(d, runge(d), &#39;-&#39;) axs[0].plot(d, pf(d), &#39;-&#39;) axs[0].set_title(&#39;Runge function, equidistant&#39;) x2 = np.array([-1, -0.8, -0.5, 0, 0.5, 0.8, 1]) pg = np.polynomial.Polynomial(interpolate(runge, x2)) axs[1].plot(x2, runge(x2), &#39;o&#39;) axs[1].plot(d, runge(d), &#39;-&#39;) axs[1].plot(d, pg(d), &#39;-&#39;) axs[1].set_title(&#39;Runge function, tail heavy&#39;) for ax in axs.flat: ax.label_outer() . . Chebyshev nodes . Chebyshev nodes are exactly those points that minimize the maximum error when increasing the degree of the polynomials. They are defined as (here)[https://en.wikipedia.org/wiki/Chebyshev_nodes]. These are the roots of the Chebyshev polynomials (of the first kind). . $$ x_k = cos left({ frac {2k-1}{2n}} pi right)$$ . def chebyshev_nodes(n): return np.array([ np.cos((2 * k - 1)/(2*n) * np.pi) for k in range(1, n+1) ]) . chebyshev_nodes(7) . array([ 9.74927912e-01, 7.81831482e-01, 4.33883739e-01, 6.12323400e-17, -4.33883739e-01, -7.81831482e-01, -9.74927912e-01]) . fig, axs = plot.subplots(2, 1) fig.set_size_inches(18.5, 10.5) x1 = np.linspace(-1, 1, 7) runge = lambda x: 1/(1+(5*x)**2) pf = np.polynomial.Polynomial(interpolate(runge, x1)) axs[0].plot(x1, runge(x1), &#39;o&#39;) axs[0].plot(d, runge(d), &#39;-&#39;) axs[0].plot(d, pf(d), &#39;-&#39;) axs[0].set_title(&#39;Runge function, equidistant&#39;) x2 = chebyshev_nodes(7) pg = np.polynomial.Polynomial(interpolate(runge, x2)) axs[1].plot(x2, runge(x2), &#39;o&#39;) axs[1].plot(d, runge(d), &#39;-&#39;) axs[1].plot(d, pg(d), &#39;-&#39;) axs[1].set_title(&#39;Runge function, Chebyshev nodes&#39;) for ax in axs.flat: ax.label_outer() . . fig, axs = plot.subplots(2, 1) fig.set_size_inches(18.5, 10.5) x1 = np.linspace(-1, 1, 10) runge = lambda x: 1/(1+(5*x)**2) pf = np.polynomial.Polynomial(interpolate(runge, x1)) axs[0].plot(x1, runge(x1), &#39;o&#39;) axs[0].plot(d, runge(d), &#39;-&#39;) axs[0].plot(d, pf(d), &#39;-&#39;) axs[0].set_title(&#39;Runge function, equidistant&#39;) x2 = chebyshev_nodes(10) pg = np.polynomial.Polynomial(interpolate(runge, x2)) axs[1].plot(x2, runge(x2), &#39;o&#39;) axs[1].plot(d, runge(d), &#39;-&#39;) axs[1].plot(d, pg(d), &#39;-&#39;) axs[1].set_title(&#39;Runge function, Chebyshev nodes&#39;) for ax in axs.flat: ax.label_outer() . . Conclusion . It&#39;s incredibly cool that with interpolating polynomials all these concepts from mathematics are tied together. For me it shows that theoretical mathematics should not be judged by its immediate utility. . Next time you take a sample think about whether uniform (equidistant) sampling actually makes sense. .",
            "url": "vanderzwaan.dev/2022/07/13/Fit-polynomials-3-Chebyshev.html",
            "relUrl": "/2022/07/13/Fit-polynomials-3-Chebyshev.html",
            "date": " • Jul 13, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Introduction to Jax",
            "content": "Jax is a relatively new library developed at Google that originated from AutoGrad to automatically get gradients of Python functions. I&#39;ve been using Jax for roughly two years and I&#39;m still excited about it and the direction it is heading in. . While it is really nice to use Jax for neural networks it is also absolutely perfect to use for other types of calculations. Especially if you want automatic gradients. . Jax by itself can be used to speed up Python code, similarly to Numba, but the distinguishing features are: . The same code can run efficiently on CPU, GPU or TPU | Automatic differentiation | Parallelization is easy | Use NumPy and SciPy as you normally would | . Jax has a limitation which I&#39;ve come to seen as also an advantage. It traces the computation instead of your code. Intially this makes it awkward as if statements don&#39;t work as expected, but this can be leveraged to make super-fast specific implementations while still maintaining a single piece of code. We&#39;ll go a little bit into how Jax works and why, but this specific topic will be covered later. . Parallelize simulations . Recently I gave a talk at Insurance Data Science about how many tasks in insurance are embarrassingly parallel and that the GPU is a great fit. While you can write your own CUDA code and make a Python wrapper...it is a pain. No particular part is hard, but getting CMake to work, get correct versions of libraries and then make a Python wrapper is just tedious and error prone. To get absolute control over memory you got to do this, as we do at my current company, but this takes a large time investment and is likely to not be worth it. . Instead I&#39;ll make a simple example with Jax and you can see how trivial it looks. . As an example we use a AR(1) mean-reverting process. We generate values $x_0,...,x_T$ where each value $x_i$ depends on it&#39;s previous value plus some noise i.e. $x_i := rho x_{i-1} + epsilon_i$ and $ epsilon sim N(0, 1)$. . The goal is to generate $N$ paths of length $T$. . N = 500 T = 120 rho = 0.9 . Generate normally distributed random values. Randomness is slightly different in Jax to make it reproducable and fit for parallel computation. It is stateless and always requires a key/state of the generator to be passed to it. For more background please see https://jax.readthedocs.io/en/latest/jax.random.html . key = jax.random.PRNGKey(1) es = jax.random.normal(key, shape=(N, T)) . WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) . Then we define a function to get the next value of $x$: . def next_x(rho, x, e): x = rho * x + e return x . next_x(0.9, 1.0, 0.033) . 0.933 . Then we define a function that generates a series of $x_1,...,x_T$. First I&#39;ll do it in vanilla Python: . def path_python(rho, es): values = np.zeros(es.shape) for i in range(1, len(es)): values[i] = next_x(rho, values[i-1], es[i-1]) return values . path_python(rho, es[0]).shape . (120,) . For this we use scan from Jax to generate a list. For good reasons we do not use for loops. Practically, Jax limits the usage :) The underlying reason and motivation is that compilers have to work hard to guess what your loop means as it can use global state. With scan we make it explicit and it&#39;s less work for a compiler. . def path(rho, es): # A small function to transform from our own definition to use the style for `scan` def _next_x(x_prev, e): x = next_x(rho, x_prev, e) return x, x_prev # Apply scan over the errors and start with x=0 _, values = jax.lax.scan( _next_x, init=0., xs=es ) return values . path(rho, es[0]).shape . (120,) . We now use vmap to parallelize over the matrix $N$ by $T$ over the first axis and pass $T$-sized vectors to the path function. Because rho is a scalar we do not map over any of its axis as it has none. . def simulate_python(rho, es): return np.vstack([path(rho, esi) for esi in es]) . simulate_python(rho, es).shape . (500, 120) . def simulate(rho, es): # We indicate how we map over the parameters: # - Rho is a scalar, so do not map over it, simply pass the same value to all # - es&#39; first dimension is the number of paths so map over the first axis return jax.vmap(path, in_axes=(None, 0))(rho, es) . simulate(rho, es).shape . (500, 120) . To get a NumPy array use np.asarray which will result in a zero-copy array: . plt.plot(np.asarray(simulate(rho, es)).T) plt.show() . Speeding up . One of our goals was to speed up the computation. We can do this with jax.jit. As a reference first time the original simulation code. . %timeit simulate(rho, es) . 57.3 ms ± 1.42 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) . fast = jax.jit(simulate) # Define a compiled version fast(rho, es) # Let it compile to exclude it from the timeit %timeit fast(rho, es) . 65.6 µs ± 677 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each) . The plain Python version takes quite a while longer. It is really not a fair comparison, but just as a reference. . %timeit simulate_python(rho, es) . 17.3 s ± 1.31 s per loop (mean ± std. dev. of 7 runs, 1 loop each) . For comparison we would have a different implementation in NumPy: . factor = np.triu(np.fromfunction(lambda i, j: j-i, (T, T))) def numpy(rho, es): rho_factor = np.triu(rho**factor) xs = es @ rho_factor return xs . %timeit numpy(rho, es) . 456 µs ± 12.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) . The End . This post went a bit into detail on how easy it is to parallelize and gave a glimpse of the different syntax of &#39;for loops&#39;. In the future I&#39;ll add more posts on the differentiation, why no loops is fine etc. .",
            "url": "vanderzwaan.dev/2022/06/28/Jax-introduction.html",
            "relUrl": "/2022/06/28/Jax-introduction.html",
            "date": " • Jun 28, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Robert a.k.a. 00011 a.k.a 1100010100000010011",
            "content": "import pandas as pd import math . Fun with compressing names which is either interesting to you as a reader or not :) It does connect to data engineering and data science: . Column storage with very similar data, check out Roaring Bitmaps | An old idea to compare music is to compress a song together with a catalogue of music from a genre. The lowest compression is then the genre. | How well data compresses tells you something about the intrisic complexity/richness of the data. Having a large dataset is a very basic measure, compression (and ideas like compression) tells you more. | Encoder/decoders kindof work like lossy compression. | . This article will be followed up with compressing timeseries. . Read census data . Read in the 1990 census data of the USA . def parse_perc(s: str) -&gt; float: return float(s.strip(&#39;%&#39;)) / 100 def parse_count(s: str) -&gt; int: return int(s.replace(&#39;,&#39;, &#39;&#39;)) df = pd.read_csv(&quot;names.csv&quot;, sep=&quot;;&quot;, converters={ &#39;Frequency (%) M&#39;: parse_perc, &#39;Frequency (%) F&#39;: parse_perc, &#39;Count M&#39;: parse_count, &#39;Count F&#39;: parse_count }) . df.tail() . Rank Name M Frequency (%) M Count M Name F Frequency (%) F Count F . 995 996 | BURL | 0.0001 | 16235 | ROBERT | 0.0001 | 25976 | . 996 997 | WALKER | 0.0001 | 16235 | ISABELLA | 0.0001 | 25976 | . 997 998 | TYREE | 0.0001 | 16235 | HERMINIA | 0.0001 | 25976 | . 998 999 | JEFFEREY | 0.0001 | 16235 | TERRA | 0.0001 | 25976 | . 999 1000 | AHMED | 0.0001 | 16235 | CELINA | 0.0001 | 25976 | . Processing the data to 2-grams . The idea is that we construct character 2-grams and list the top 4 letters following a letter. For example, Jefferey would result in [je, ef, ff, fe, er, re, ey]. The leter e is the start of three 2-grams: ef, er and ey. By summing up the occurences of each 2-gram we can find out what&#39;s the most likely character following a specified letter. . Below we select the top 4: . top = 4 def two_grams(s: str): return list( zip(s[:-1], s[1:])) # Construct flattened list of weighted 2-grams and get the top 8 per letter weighted_two_grams = pd.DataFrame( df.apply( lambda row: [(a,b, row[&#39;Frequency (%) M&#39;]) for a,b in two_grams(row[&#39;Name M&#39;])], axis=1 ).apply(pd.Series).stack().to_list(), columns=[&#39;first&#39;, &#39;second&#39;, &#39;freq&#39;] ).groupby(by=[&#39;first&#39;, &#39;second&#39;]).sum().sort_values(by=&#39;freq&#39;, ascending=False).groupby(by=&#39;first&#39;).head(top).sort_index() weighted_two_grams.head() . freq . first second . A L 0.0549 | . M 0.0725 | . N 0.1032 | . R 0.1251 | . B E 0.0482 | . Encoding names . To simplify we assume we only have 26 letters. To encode names we go through the name and output a (binary) code for each letter. We distinguish two types of codes: . The number of the letter in the alphabet (1-26) | The position in the top-4 of letters of the letter preceding this one | Let&#39;s define some functions that do this for us! . def encode_char(c, prev): if prev is not None and c in weighted_two_grams.loc[(prev,)].index: return (0, weighted_two_grams.loc[(prev,)].index.get_loc(c)) else: return (1, ord(c)-65) def encode(name): encoded = [] prev = None for c in name: code = encode_char(c, prev) prev = c encoded.append(code) return encoded def decode_char(code, prev): t, nr = code if prev is not None and t == 0: return weighted_two_grams.loc[(prev,)].iloc[nr].name else: return chr(nr+65) def decode(codes): decoded = [] prev = None for code in codes: prev = decode_char(code, prev) decoded.append(prev) return &#39;&#39;.join(decoded) . As an example let&#39;s see how robert is encoded and decoded: . encode(&quot;ROBERT&quot;) . [(1, 17), (0, 2), (0, 0), (0, 0), (0, 2), (0, 3)] . decode(encode(&quot;ROBERT&quot;)) . &#39;ROBERT&#39; . How much bits do we need? . We could encode the two types of codes with a 0 followed by 2 bits or 1 followed by 5 bits. The two bits give the position in the top 4, and the 5 bits give the position in the 26 letter alphabet (2^5=32). There might be some room to use even less bits with some trickery. . The 2-gram lookup table requires 2645=520 bits: sequentially store the top 4 for a...z, and there is no top four pad with zeros). . Suppose we encounter the top 1000 names proportionally than on average we need 21.92 bits per name: . def nr_bits(codes): return sum([ 1 + (math.log2(top) if t == 0 else 5) for t, nr in codes]) . df[&#39;enc&#39;] = df[&#39;Name M&#39;].map(lambda x: nr_bits(encode(x))) (df[&#39;enc&#39;] * df[&#39;Frequency (%) M&#39;]).sum() . 21.919200000000004 . Robert would be encoded as . def to_bin(encoding): return &#39;&#39;.join([str(code) + format(nr, &#39;b&#39;) for code, nr in encoding]) . to_bin(encode(&#39;ROBERT&#39;)) . &#39;1100010100000010011&#39; . Can we do better? . If the first 1000 first names really cover 90% we can also encode the first 1000 names in a fixed list. We then encode it as follows: . Single bit 0 if not in the list and 1 if in the list | If in the list 10 bits a number &lt;1000 | If not in the list 5 bits per letter | . The table takes 5677 * 5 = 28385 bits to store. On average this takes 10 bits per name. . The break-even point is at roughly encoding 2300 names from the top 1000. . (28385-520)/12 . 2322.0833333333335 . Can we do even better? . Probably the best idea is to mix the frequency table approach and encoding the top 200 names, as these already cover 72% of all names. The frequency table is very small and also works for names outside of the top 1000. . df.sort_values(by=&#39;Frequency (%) M&#39;, ascending=False).head(200)[&#39;Frequency (%) M&#39;].sum() . 0.7261 . This is left as an exercise for the reader. .",
            "url": "vanderzwaan.dev/2022/05/25/Compression-of-names.html",
            "relUrl": "/2022/05/25/Compression-of-names.html",
            "date": " • May 25, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Linear regression and the power of two",
            "content": "In linear regression we want to find a linear relation between explanatory variables and a response value. For example we want to find out how the house price (=response value) relates to the size of the house and it&#39;s location (=explanatory variables). . The response value has to be a continous value like house prices. (There is a variation called logistic regression for discrete outcomes). . Linear regression is typically the first estimation method you learn. But why do we minimize the squared sum of differences between the observed responses and the responses of the linear approximation? (This is also called Least Squares) . $$ min_c sum left( ( underbrace{c_0 + c_1 cdot x}_{ text{Approximation}} ) - underbrace{y}_{ text{Response}} right)^2 $$ . Especially outside math/computer science/econometry courses this is handwaved as &quot;Two is a good compromise&quot;. Indeed it is, it has to property that all differences (or errors) are positive. This means that an underestimation does not cancel out an overestimation. Additionally, it ensures that many small errors are preferred to few large errors. . Perhaps you want to know a better answer. Then read this. Otherwise don&#39;t :) It turns out if we take a detour through random variables we derive the 2. . Rephrasing with random variables . A trick that is often used is to declare that there is a random unobserved factor in play. Even though the process might be completely deterministic. You can argue that horrendous deterministic complexity is random to a simple observer, like a linear approximation of a high dimension function. . We can then rephrase the questions as: find a linear relation between explanatory variables and a response value that minimizes the unexplained random factor: . $$ y_i = c_0 + c_1 cdot x_i + epsilon_i $$ . The previous approach to minimize the squared sum of errors is the same as minimizing the sum of squared unexplained random factors $ epsilon$. . To work with this randomness we need to know or assume on how the randomness behaves. Typically we assume the error is independent identically normally distributed (Gaussian). . This is not always reasonable but it is a very good starting point. Independence and identical are quite often true (enough). Because of the Central Limit Theorem we know that a sum of random variables converges to a normal distribution. . Maximum likelihood . Maximum likelihood is a method of estimating the parameters of an unknown random distribution. Distribution is just a word to describe the possible values and their chances the random variable takes. In our case we are interested in estimating the mean ($ mu$) and standard deviation ($ sigma$) of the random factor $ epsilon$. . The possible values and their changes (likelihood) of occuring can be described by the probability density function (PDF). This function caputures this information. The CDF is the cumulative version. . There is a slight tricky notion that $pdf(5)$ is not the probablity that the random variable has value 5, but the density. The random variable is continuous and there are infinite values in any interval. However the integral (weighted sum) $ int_{4 &lt; x &lt; 5} pdf(x) d x$ is the probability that x is between 4 and 5. . If this confuses you just assume density is almost the probability. . The maximum likelihood of values $x$ occuring is defined as multiplying all densities for each individual $x$. If you read probability instead of density you get the intuition that his quantity relates to the probability of all $x$ occuring simultaneously. Maximimizing this quantity amounts to maximizing the probability. . So...how does this work out? . Using maximum likelihood we want to maximize the likelihood that $y_i$ occurs given $x_i$ with parameters $c_0, c_1$ and $ sigma$: . $$ LH(y | x; c_0, c_1, sigma) = prod_i pdf(y_i | x_i; c_0, c_1, sigma) $$ . The PDF of the normal distribution is defined as $$ pdf(x; mu, sigma) = frac{1}{ sigma sqrt(2 pi)} exp left[ - frac{1}{2} left( frac{x - mu}{ sigma} right)^2 right] $$ . We then get: $$ begin{aligned} LLH(y | x; c_0, c_1, sigma) &amp;= sum_i log pdf(y_i | x_i; c_0, c_1, sigma) &amp;= sum_i log left( frac{1}{ sigma sqrt(2 pi)} exp left[ - frac{1}{2} left( frac{ left( c_0 + c_1 cdot x_i right) - y_i}{ sigma} right)^2 right] right) &amp;= sum_i log frac{1}{ sigma sqrt(2 pi)} + left( - frac{1}{2} left( frac{ left( c_0 + c_1 cdot x_i right) - y_i}{ sigma} right)^2 right) end{aligned} $$ . We can already see that, ignoring $ sigma$, maximizing LLH is equivalent to minimizing least squares. In least squares we did not optimize over $ sigma$. To show it also works out if $ sigma$ would be optimized over we show that the estimation of $ sigma$ depends on $c_0$, $c_1$, $x$ and $y$. . Feel free to skip the last step as it requires more algebra. . Note: Before we mentioned that the error has a mean $ mu$ and standard deviation $ sigma$. In our case $ mu$ is superfluous as if it would be non-zero we would set $c_0 := c_0 + mu$ and $ mu := 0$ and all calculations would work out the same. . Last step . We show here that the estimation of $ sigma$ is defined in terms of other parameters and the variables, and is simply the sample standard deviation: . $$ sigma^2 = frac{1}{n} sum_i ((c_0 + c_1 cdot x_i) - y_i)^2 $$ . The optimal set of parameters that maximize $LLH$ has the property that the partial derivates are zero: . $$ begin{aligned} frac{ partial LLH}{ partial c_0} &amp;= 0 = - frac{1}{ sigma^2} sum_i (c_0 + c_1x_i-y_i) frac{ partial LLH}{ partial c_1} &amp; = 0 = - frac{1}{ sigma^2} sum_i (c_0 x_i + c_1x_i^2-x_iy_i) end{aligned} $$Then $$ begin{aligned} frac{ partial LLH}{ partial sigma} &amp;= - frac{n}{2 sigma^2} + left( frac{1}{2} sum_i left( left( c_0 + c_1 cdot x_i right) - y_i right)^2 right) left( frac{1}{( sigma^2)^2} right) &amp;= frac{1}{2 sigma^2} left( frac{1}{ sigma^2} sum_i left( left( c_0 + c_1 cdot x_i right) - y_i right)^2 - n right) end{aligned} $$ Which is only zero if $$ sigma^2 = frac{1}{n} sum_i left( left( c_0 + c_1 cdot x_i right) - y_i right)^2 $$ . Conclusion . Now you know 2. .",
            "url": "vanderzwaan.dev/2022/05/11/Linear-regression-and-the-power-of-two.html",
            "relUrl": "/2022/05/11/Linear-regression-and-the-power-of-two.html",
            "date": " • May 11, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Fit polynomials 2",
            "content": "Introduction . This is the second part of &#39;Fit with polynomials&#39;. Check out the first part or read on about Lagrange polynomials and solving with derivatives. For this post we assume we know the underlying function $f(x)$ and no longer content with just samples. . Example inputs . We can either be given some points $p_1,...,p_n$ or be given a function $f()$ that we want to approximate. I use these interchangably, because a function is convenient as ground truth and to approximate a function we will be sampling from that function. . To make it concrete lets use some functions and sample 5 points $x_1,...,x_5$ from $[-1, +1]$. . d = np.linspace(-1, 1, 101) # I pick an odd number since linspace then also includes 0 (the middle value) x = np.linspace(-1, 1, 5) . fig, axs = plt.subplots(2, 2) fig.set_size_inches(18.5, 10.5) f = lambda x: x**2 axs[0, 0].plot(x, f(x), &#39;o&#39;) axs[0, 0].plot(d, f(d), &#39;-&#39;) axs[0, 0].set_title(&#39;Quadratic&#39;) g = lambda x: 1/(1+(5*x)**2) axs[1, 0].plot(x, g(x), &#39;o&#39;) axs[1, 0].plot(d, g(d), &#39;-&#39;) axs[1, 0].set_title(&#39;Runge function&#39;) h = lambda x: x**14-x**3 axs[0, 1].plot(x, h(x), &#39;o&#39;) axs[0, 1].plot(d, h(d), &#39;-&#39;) axs[0, 1].set_title(&#39;Recliner&#39;) for ax in axs.flat: ax.label_outer() . . Legendre instead of Vandermonde . The choice of de Vandermonde matrix seems natural as this is the natural way we denote polynomial $p(x) = c_0 cdot x^0 + c_1 cdot x^1 ... c_n cdot x^n$. In the previous post we phrased the interpolation (or approximation) as finding the coefficients $c$. . Another way to look at it was to say that we interpolate points with a linear combination of polynomials $p_1(x)=x^0, ..., p_n(x)=x^n$. It is mere semantics but phrasing it this way helps to understand Legendre polynomials. Instead of using the monomials we get a different set of polynomials. (Wikipedia on Legendre polynomials). . This set of polynomials should have the property that any polynomial of degree $ leq n$ can be represented as a linear combination of polynomials from this set. We say that the polynomials are a basis. Secondly, the set shouldn&#39;t be too large. . The set of Legendre polynomials is a basis for the interval $[-1, 1]$ and the set of degree $n$ is orthogonal and contains exactly $n+1$ polynomials, the same as for the monomials. Orthogonal means that no Legendre polynomial is a linear combination of multiple other Legendre polynomials: we need them all. . Definition . Since this post is only about Legendre polynomials we denote $P_n(x)$ as the Legendre polynomial of degree $n$, and $P_0,...,P_n$ form a basis for degree $n$ polynomials. This means we can represent any polynomial with a linear combination of Legendre polynomials, specifically any polynonomial of degree $n$ as a linear combination of Legendre polynomials $P_0,...P_n$. . An additional property is that the following is true: . $$ int_{-1}^1 P_m(x)P_n(x) d x = 0, textrm{ if $n neq m$ } $$ . Definition 2 (As definition 1 us exceedingly unhelpful?) . Wow. The above probably did not help at all. We should construct these polynomials exactly and give examples. Hold on to your jacket, we&#39;re about to give complicated formulaes. . The easiest is probably derived from the Rodrigues&#39; formula: . $$P_n(x) = sum_{k=0}^n binom{n}{k} binom{n+k}{k} left( frac{x-1}{2} right)^k$$ . Ok. Let&#39;s just start plotting these things :) . Implementing Legendre polynomials . First we define the function naively: . def legendre_naive(n): def _legendre(x): total = 0 for k in range(n+1): total += math.comb(n, k) * math.comb(n+k, n) * ((x-1)/2)**k return total return _legendre . fig, ax = plt.subplots() leg2 = legendre_naive(2) y = [leg2(xi) for xi in d] series = plt.plot(d, y) . . We should have a nicer function where we can pass in an array. So lets try again with some slicing trickery: . def legendre(n): def _legendre(x): # Create a (1, k)-array k = np.arange(n+1)[None, :] # Turn x into a (|x|, 1)-array. The result is a (|x|, k)-array, and sum over the second axis (the k) parts = scipy.special.comb(n, k) * scipy.special.comb(n+k, n) * ((x[:, None]-1)/2)**k return np.sum(parts, axis=1) return _legendre . fig, ax = plt.subplots() leg2 = legendre_naive(2) y = [leg2(xi) for xi in d] series = plt.plot(d, y) series = plt.plot(d, legendre(2)(d)) . . Visualize: squiggly vs boring . Now we plot the Legendre polynomials and their monomial counterpart. Tl;dr is that Legendre polynomials are squiggly. . fig, axs = plt.subplots(2, 2) fig.set_size_inches(18.5, 10.5) axs[0, 0].plot(d, legendre(2)(d), &#39;-&#39;, label=&quot;Legendre&quot;) axs[0, 0].plot(d, d**2, &#39;-&#39;, label=&quot;Monomial&quot;) axs[0, 0].set_title(&#39;Degree 2&#39;) axs[0, 0].legend() axs[1, 0].plot(d, legendre(3)(d), &#39;-&#39;, label=&quot;Legendre&quot;) axs[1, 0].plot(d, d**3, &#39;-&#39;, label=&quot;Monomial&quot;) axs[1, 0].set_title(&#39;Degree 3&#39;) axs[1, 0].legend() axs[0, 1].plot(d, legendre(5)(d), &#39;-&#39;, label=&quot;Legendre&quot;) axs[0, 1].plot(d, d**5, &#39;-&#39;, label=&quot;Monomial&quot;) axs[0, 1].set_title(&#39;Degree 5&#39;) axs[0, 1].legend() axs[1, 1].plot(d, legendre(10)(d), &#39;-&#39;, label=&quot;Legendre&quot;) axs[1, 1].plot(d, d**10, &#39;-&#39;, label=&quot;Monomial&quot;) axs[1, 1].set_title(&#39;Degree 10&#39;) axs[1, 1].legend() for ax in axs.flat: ax.label_outer() . . Solving . Again, matrices . In the previous post we found the coefficients by first representing the Vandermonde polynomial as a matrix $L(x)$ and then solving $L(x)c=f(x)$. . To solve the interpolation of points $x$ we used the Vandermonde matrix. For the function $f(x) = x^2$ the coefficients are $c_0=0, c_1=0, c_2=1$ as $x^2=0 cdot x^0 + 0 cdot x^1 + 1 cdot x^2$. This means that if we do $V_2(x) cdot c$ we should get $f(x)$ (where $V$ is the Vandermonde matrix). Let&#39;s do this: . V = np.vander(x, N=3, increasing=True) c = np.array([0., 0., 1.]) np.dot(V, c) . array([1. , 0.25, 0. , 0.25, 1. ]) . So far so good. NumPy has similar functionality for Legendre polynomials. The first three Legendre polynomials are $P_0(x) = 1$, $P_1(x) = x$ and $P_2(x) = (3x^2-1)/2$. . We expect the coefficients to be $$c_0= frac{1}{3}, c_1=0, c_2= frac{2}{3}$$ as $$ frac{1}{3} cdot 1 + 0 cdot x + frac{2}{3} cdot left( frac{1}{2}(3x^2-1) right) = frac{1}{3} + x^2 - frac{1}{3} = x^2 $$ . L = np.polynomial.legendre.legvander(x, 2) c = np.array([1/3, 0, 2/3]) np.dot(L, c) . array([1. , 0.25, 0. , 0.25, 1. ]) . We can verify this with solving it directly: . np.linalg.lstsq(L, f(x), rcond=None)[0] . array([0.33333333, 0. , 0.66666667]) . Great! . Wait, what? So, how did this help? . You might be wondering how this has helped us. Instead of an intuive description of polynomials by monomials (Vandermonde matrix) some integrals and weird Legendre polynomials were forced on you. . Up to this point it actually didn&#39;t help. Much. We can hope that the Legendre matrix is more stable than Vandermonde*, but that is not the crux. The real beauty will come when we look at derivates and something magical will happen. . *I think they&#39;re both unstable/high condition number, but don&#39;t know for sure. . Derivatives to the rescue . Previously we solved the interpolation of a function $f()$ by: . Sampling some points $x$ and computing $f(x)$ | Creating a Vandermonde or Legendre matrix $A$ | (Approximately) solve $Ac=f(x)$ i.e. find a $c$ such that $||Ac-f(x)||_2$is minimized | Profit | We can rephrase the minimization with Legendre polynomials as: . $$ F(c) = int_{-1}^1 left( f(x) - sum_{k=0}^n c_k L_k(x) right)^2 dx$$ . This function has a minimum (just believe this) and so the minimum is where the partial derivatives are zero. . $$ frac{ partial F}{ partial c_k}=0 textrm{ for } k=1,...n $$ . For the exact interpolation we solved $Ac=f(x)$ which could be phrased in matrix notation. Guess what we&#39;re going to do. . Bla bla system of equations with derivatives . As an example we take the five points $x$ and degree $2$, which means we have $c_0, c_1, c_2$ as variables. For . $$r(c)=p(q(c))$$the derivative is $$r&#39;(c) = p&#39;(q(c))q&#39;(c)$$ . Set $$ q(c_0) = f(x) - sum_{k=0}^n c_k L_k(x) = f(x) - sum_{k=1}^n c_k L_k(x) - c_0 L_0(x)$$ $$ p(x) = x^2 $$ . The partial derivative of $q$ is then quite simple: . $$ frac{ partial q}{ partial c_0} = - L_0(x) $$ . We then get $$ frac{ partial F}{ partial c_0} = 2 int_{-1}^1 left( f(x) - sum_{k=0}^n c_k L_k(x) right) cdot -L_0(x) dx $$ . And in general we can state that $$ frac{ partial F}{ partial c_p} = -2 int_{-1}^1 left( f(x) - sum_{k=0}^n c_k L_k(x) right) cdot L_p(x) dx $$ . Setting these to zero we get a system of equations: . $$ begin{aligned} c_0 int_{-1}^1 L_0(x) cdot L_0(x) dx &amp;+ c_1 int_{-1}^1 L_0(x) cdot L_1(x) dx &amp;+ c_2 int_{-1}^1 L_0(x) cdot L_2(x) dx &amp;= int_{-1}^1 L_0(x) f(x) dx c_0 int_{-1}^1 L_1(x) cdot L_0(x) dx &amp;+ c_1 int_{-1}^1 L_1(x) cdot L_1(x) dx &amp;+ c_2 int_{-1}^1 L_1(x) cdot L_2(x) dx &amp;= int_{-1}^1 L_1(x) f(x) dx c_0 int_{-1}^1 L_1(x) cdot L_0(x) dx &amp;+ c_1 int_{-1}^1 L_2(x) cdot L_1(x) dx &amp;+ c_2 int_{-1}^1 L_2(x) cdot L_2(x) dx &amp;= int_{-1}^1 L_2(x) f(x) dx end{aligned} $$ Hitting pay dirt . (Yes I quoted &#39;Gold Rush&#39;) . Remember the seemingly useless statement about orthogonality? This is where the beautiful part starts! Legendre polynomials are orthogonal and this is equivalent to: . $$ int_{-1}^1 P_m(x)P_n(x) d x = 0, textrm{ if $n neq m$ } $$ . If we look at the system of equations above we see that all entries except the diagonal are orthogonal and we have a diagonal matrix! . $$ begin{aligned} c_0 int_{-1}^1 L_0(x) cdot L_0(x) dx &amp; &amp;&amp;= int_{-1}^1 L_0(x) f(x) dx &amp; c_1 int_{-1}^1 L_1(x) cdot L_1(x) dx &amp;&amp;= int_{-1}^1 L_1(x) f(x) dx &amp; &amp; c_2 int_{-1}^1 L_2(x) cdot L_2(x) dx &amp;= int_{-1}^1 L_2(x) f(x) dx end{aligned} $$ Let us denote this matrix as $G$ and the right-hand-side as $y(x)$, which means that to solve the equation $Gc=y(x)$ the matrix $G$ is it&#39;s own inverse! We can simply compute $Gy(x)$ to solve for $c$. . Even better, because $G$ is diagonal we can simply solve $c_i = y(x_k) / G(x)_k$ for each $k$ in just a few operations. But wait, there is more! Turns out that $G(x)_k = frac{2}{2k+1}$. so we only need to solve . $$ c_k = y(x_k) / frac{2}{2k+1} $$ . def G(n): return np.array([2/(2 * k + 1) for k in range(n+1)]) . Deriving $G_k = frac{2}{2k+1}$ . For the interested people. The recursive defintion of $P_k$ is (dropping $x$ for brevity): . $$ P_k = ((2k-1)x P_{k-1}(x) - (k-1) P_{k-2}(x))/k $$ . with $$ P_1 = x $$ . The idea is that we substitute and use orthogonality: . $$ begin{aligned} F_k = int_{-1}^1 P_k cdot P_k dx &amp;= k^{-1} int_{-1}^1 ((2k-1) x cdot P_{k-1} cdot P_{k}) - (k-1) cdot P_{k-2} cdot P_{k} dx &amp;= frac{2k-1}{k} int_{-1}^1 x cdot P_{k-1} cdot P_{k} dx - frac{k-1}{k} cdot int_{-1}^1 P_{k-2} cdot P_{k} dx &amp;= frac{2k-1}{k} int_{-1}^1 x cdot P_{k-1} cdot P_{k} dx end{aligned} $$because $P_k cdot P_q = 0$ for $k neq q$. . During rewriting I didn&#39;t quickly see how to continue. I was trying to find a recursive definition $F_k = textrm{something} cdot F_{k-1}$ and didn&#39;t think to substitute $P_k$ with a higher $P_{k+1}$. On MathExchange there was the idea to substitute . $$x P_{k} = left( (k+1)P_{k+1} + k P_{k-1} right) / (2k+1)$$ . In hindsight this makes sense as we are continuously using the orthogonality to cancel out products of $P$. . Using this idea we obtain . $$ begin{aligned} F_k &amp;= frac{2k-1}{k} int_{-1}^1 x cdot P_{k} cdot P_{k-1} dx &amp;= frac{2k-1}{k} int_{-1}^1 P_{k-1} cdot left( (k+1)P_{k+1} + k P_{k-1} right) / (2k+1) dx &amp;= frac{2k-1}{k} cdot frac{k+1}{2k+1} int_{-1}^1 P_{k-1} cdot P_{k+1} dx + frac{2k-1}{2k+1} int_{-1}^1P_{k-1}^2 dx) &amp;= frac{2k-1}{2k+1} F_{k-1} &amp;= frac{2k-1}{2k+1} cdot frac{2k-3}{2k-1} F_{k-2} &amp;= frac{1}{2k+1} F_{0} &amp;= frac{2}{2k+1} end{aligned} $$because $$F_{0} = int_{-1}^1 P_0(x)^2 dx = int_{-1}^1 1 dx = 2$$ . Computing the right hand side $y(x)$ . Remaining is computing the integral over the function $f$ and Legendre polynomials. Deriving a closed form is too much effort, we will simply approximate the integral as it is basically a sum. . Assuming evenly spaced points $x$ the integral can be approximated as the sum $f(x)$ divided by $|x|$ times $2$ (the size of the interval $[-1, 1]$). . def y_approx(f, d, precision=101): x = np.linspace(-1, 1, precision) legendres= [legendre(k) for k in range(d+1)] def lv(x): return np.stack([leg(x) for leg in legendres]) return np.sum(f(x) * lv(x), axis=1)/precision*2 . Alternatively we can use the trapezoidal rule and use a NumPy function: . def y(f, d, precision=101): x = np.linspace(-1, 1, precision) legendres= [legendre(k) for k in range(d+1)] def lv(x): return np.stack([leg(x) for leg in legendres]) return np.trapz(f(x) * lv(x), dx=2/precision, axis=1) . Checking the results we see that the approximation with trapezoidal rule is pretty good! . Define a degree 2 Legendre polynomial . L = np.polynomial.legendre.legvander(x, 2) . Check the results with function $f(x)=x^2$: . c1 = y_approx(f, 2)/G(2) np.dot(L, c1) . array([ 1.050396 , 0.2512005, -0.015198 , 0.2512005, 1.050396 ]) . c2 = y(f, 2)/G(2) np.dot(L, c2) . array([ 9.90990059e-01, 2.47487629e-01, -3.46514851e-04, 2.47487629e-01, 9.90990059e-01]) . Recall that we found the solution c3 by hand: . c3 = np.array([1/3, 0, 2/3]) np.dot(L, c3) . array([1. , 0.25, 0. , 0.25, 1. ]) . Summary . Hope you appreciated the beautiful way how fitting a polynomial through Legendre polynomials turned out to require hard calculations. The coefficients can be found with a simple formula, which only requires to evaluate an integral ($y(x_k)$), which can be done numerically: . $$ c_k = y(x_k) / frac{2}{2k+1} $$ . For the visualize included feast your eyes: . d = np.linspace(-1, 1, 101) # I pick an odd number since linspace then also includes 0 (the middle value) x = np.linspace(-1, 1, 5) fig, axs = plt.subplots(2, 2) fig.set_size_inches(18.5, 10.5) f = lambda x: x**2 c_f = y(f, 2)/G(2) L_f = np.polynomial.legendre.Legendre(c_f, [-1, 1]) axs[0, 0].plot(d, L_f(d), &#39;-&#39;, label=&quot;Legendre(2)&quot;) axs[0, 0].plot(d, f(d), &#39;-&#39;, label=&quot;Original&quot;) axs[0, 0].set_title(&#39;Quadratic&#39;) axs[0, 0].legend() g = lambda x: 1/(1+(5*x)**2) c_g = y(g, 9)/G(9) L_g = np.polynomial.legendre.Legendre(c_g, [-1, 1]) axs[1, 0].plot(d, L_g(d), &#39;-&#39;, label=&quot;Legendre(9)&quot;) axs[1, 0].plot(d, g(d), &#39;-&#39;, label=&quot;Original&quot;) axs[1, 0].set_title(&#39;Runge function&#39;) axs[1, 0].legend() h = lambda x: x**14-x**3 c_h = y(h, 5)/G(5) L_h = np.polynomial.legendre.Legendre(c_h, [-1, 1]) axs[0, 1].plot(d, L_h(d), &#39;-&#39;, label=&quot;Legendre(5)&quot;) axs[0, 1].plot(d, h(d), &#39;-&#39;, label=&quot;Original&quot;) axs[0, 1].set_title(&#39;Recliner&#39;) axs[0, 1].legend() for ax in axs.flat: ax.label_outer() . . References . https://math.stackexchange.com/questions/200924/why-is-lagrange-interpolation-numerically-unstable | http://www.math.pitt.edu/~sussmanm/2070/lab_09/index.html | https://www.math.usm.edu/lambers/mat772/fall10/lecture5.pdf | https://en.wikipedia.org/wiki/Runge%27s_phenomenon | https://en.wikipedia.org/wiki/Legendre_polynomials | https://en.wikipedia.org/wiki/Lagrange_polynomial | https://epubs.siam.org/doi/pdf/10.1137/S0036144502417715 | Other posts here: . Chebyshev nodes to avoid oscillation in part 3 | Solve $ geq$ invert in this article | .",
            "url": "vanderzwaan.dev/2022/05/06/Fit-polynomials-2.html",
            "relUrl": "/2022/05/06/Fit-polynomials-2.html",
            "date": " • May 6, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Matrix inversion or solve",
            "content": "The short advice is always to solve for $x$ in linear equations $Ax=y$ with (for example) LU-decomposition, and don&#39;t do $A^{-1}y$. By why exactly? . This post came to be after reading Why Shouldn&#39;t I Invert That Matrix by Gregory Gundersen. He does an exellent job on describing matrix inversion, LU-decomposition and (ill) conditioned matrices. I&#39;ll repeat some terms here. . The argument is that solving is faster and more precise than inversion: . LU decomposition requires less floating point operations than (plain) matrix inversion. | Floating point operations are less precise, so more lead to larger errors. | . In this post I use matrices related to polynomial interpolation, including Hilbert matrices. Hilbert matrices are ill-conditioned, so we expect that inversion will be bad. . Precision and floats . I&#39;ll not go into the computational complexity, read the above post for a nice explanation. We are not solving symbolically but with 32 (or 64) bit floats. For example $0.1$ cannot be represented as a float: see this article. The more operations we execute the more potential there is for errors. . In mathematics both methods are precise if we solve symbolically. In mathematics there is the condition number of matrices, a higher condition number means that small changes in $A$ lead to large changes in $x$. This seems exactly the intuition that we need to explore under what conditions we can expect more errors, and the difference between two methods is more pronounced. . Solve $Ax=y$ . We can solve $Ax=y$ with the LU-decomposition $A=LU$ and then solve $Lz=x$ and then $Ux = z$. Since $L$ and $U$ are triangular matrices this is straightforward. With triangular matrices you can start with an equation with one variable, solve it and continue. There&#39;ll always be an equation on a single variable (that you didn&#39;t solve yet). . Finding the decomposition is the tricky part. The NumPy function solve does this with the method from LAPACK. We create an example from polynomial interpolation of the function $f(x)=x^2$ on the points $ {- frac{1}{2},0, frac{1}{2} }$: . f = lambda x: x**2 x = np.array([-.5, 0., .5]) A = np.vander(x, increasing=True) . For a small number of points this gives the same results: . np.linalg.solve(A, f(x)) - np.linalg.inv(A) @ f(x) . array([0., 0., 0.]) . But for larger matrices it starts to differ: . x = np.linspace(-1, 1, 40) A = np.vander(x, increasing=True) . np.linalg.solve(A, f(x)) - np.linalg.inv(A) @ f(x) . array([-1.40100652e-01, 1.11382317e-01, 1.00881039e-01, 4.94520867e-01, 3.78189174e-01, -6.48834908e+00, -3.71602241e+00, 2.78155158e+01, 9.44989685e+00, -7.18503682e+01, -2.39184816e-01, 1.33213768e+02, -2.37054964e+01, -1.84869925e+02, 2.41140115e+01, 1.66567935e+02, 2.94400070e+00, -6.93285557e+01, -2.55111985e+01, 1.67702176e+00, 3.48501630e+01, -2.03380240e+01, -3.16823477e+01, 4.88047696e+01, 1.50645974e+01, -3.87264745e+01, 3.73249968e-01, 1.76218177e+01, -2.31686235e+00, -5.51101135e+00, -1.84854387e+00, 4.96125557e-01, 2.44568978e+00, 1.08034577e+00, -1.51065340e+00, -8.77657212e-01, 4.59726355e-01, 3.29110196e-01, -5.81216797e-02, -4.90222286e-02]) . Also the speeds are different: . %timeit np.linalg.inv(A) @ f(x) . 111 µs ± 13.5 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each) . %timeit np.linalg.solve(A, f(x)) . 93 µs ± 11.8 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each) . Hilbert . The difference between the two method is due to the number of floating point operations and the amount of error introduced. We will show this with the Hilbert matrix as an example. The Hilbert matrix is ill conditioned, small changes in the matrix will lead to large changes in the computed output. . Note that the Vandermonde matrix above is almost the Hilbert matrix. . Define the Hilbert matrix . def Hilbert(n): return np.array([ [(1/(i+j-1)) for j in range (1, n+1)] for i in range (1, n+1)]) Hilbert(5) . array([[1. , 0.5 , 0.33333333, 0.25 , 0.2 ], [0.5 , 0.33333333, 0.25 , 0.2 , 0.16666667], [0.33333333, 0.25 , 0.2 , 0.16666667, 0.14285714], [0.25 , 0.2 , 0.16666667, 0.14285714, 0.125 ], [0.2 , 0.16666667, 0.14285714, 0.125 , 0.11111111]]) . The inverse of the Hilbert matrix is known in closed form (see https://en.wikipedia.org/wiki/Hilbert_matrix): . def Hilbert_inv(n): return np.array([ [ (-1)**(i+j) * (i+j-1) * math.comb(n+i-1, n-j) * math.comb(n+j-1, n-i) * math.comb(i+j-2, i-1)**2 for j in range (1, n+1)] for i in range (1, n+1)]) Hilbert_inv(5) . array([[ 25, -300, 1050, -1400, 630], [ -300, 4800, -18900, 26880, -12600], [ 1050, -18900, 79380, -117600, 56700], [ -1400, 26880, -117600, 179200, -88200], [ 630, -12600, 56700, -88200, 44100]]) . The inverse of the inverse should be the same as the original: . np.all(np.isclose(Hilbert(10), np.linalg.inv(Hilbert_inv(10)))) . True . Error inversion vs solve . Define the error as the mean squared difference between solving through taking the inverse and solving throuh LU-decomposition: . rng = default_rng() def error(n): # Create the matrix A and solution y A = Hilbert(n) x = rng.uniform(size=(n,1)) y = A @ x # Solve Ax=y x_inverse = np.linalg.inv(A)@y x_solve = np.linalg.solve(A, y) e_inverse = np.mean((x - x_inverse)**2) e_solve = np.mean((x - x_solve)**2) return np.stack((e_inverse, e_solve)) . You can see that the difference is quite pronounced for large $n$ (chart is in log-scale) . fig, ax = plt.subplots() xs = np.arange(10, 200, 10) ys = [error(x) for x in xs] series = plt.plot(xs, ys) plt.legend(iter(series), (&#39;Inverse&#39;, &#39;Solve&#39;)) ax.set_yscale(&quot;log&quot;, nonpositive=&#39;clip&#39;) . Error . Additionally we can look at the quality of the inverse and you can see that the difference increases sharply for larger matrices (chart is in log-scale): . def error_inv(n): return np.mean((Hilbert_inv(n) - np.linalg.inv(Hilbert(n)))**2) fig, ax = plt.subplots() xs = np.arange(10, 50, 10) ys = [error_inv(x) for x in xs] plt.plot(xs, ys) ax.set_yscale(&quot;log&quot;, nonpositive=&#39;clip&#39;) .",
            "url": "vanderzwaan.dev/2022/02/15/Matrix-inversion.html",
            "relUrl": "/2022/02/15/Matrix-inversion.html",
            "date": " • Feb 15, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Fit polynomials 1",
            "content": "Introduction . Fitting with polynomials is the natural step-up from linear regression: fitting with a line. This article is on interpolating series and functions with polynomials, both exact and approximately. . What started as a small idea to generate a moving image of a polynomial chasing random points has evolved in a tour through nice mathematics. There is both code and mathematics so pick whatever suits you. I hope you enjoy it! . We start with defining the problem and some notation. No worries, we also add graphs to make the problem explicit. . Example inputs . We can either be given some points $p_1,...,p_n$ or be given a function $f()$ that we want to approximate. I use these interchangably, because a function is convenient as ground truth and to approximate a function we will be sampling from that function. . To make it concrete lets use some functions and sample 5 points $x_1,...,x_5$ from $[-1, +1]$. Additionally, we use the diabetes dataset from SciKit. . d = np.linspace(-1, 1, 101) x = np.linspace(-1, 1, 5) . fig, axs = plot.subplots(2, 2) fig.set_size_inches(18.5, 10.5) f = lambda x: x**2 axs[0, 0].plot(x, f(x), &#39;o&#39;) axs[0, 0].plot(d, f(d), &#39;-&#39;) axs[0, 0].set_title(&#39;Quadratic&#39;) g = lambda x: 1/(1+(5*x)**2) axs[1, 0].plot(x, g(x), &#39;o&#39;) axs[1, 0].plot(d, g(d), &#39;-&#39;) axs[1, 0].set_title(&#39;Runge function&#39;) h = lambda x: x**14-x**3 axs[0, 1].plot(x, h(x), &#39;o&#39;) axs[0, 1].plot(d, h(d), &#39;-&#39;) axs[0, 1].set_title(&#39;Recliner&#39;) DX, y = load_diabetes(return_X_y=True) dx = DX[:,2] # Map to [-1,1] dx = (dx - min(dx)) / (max(dx) - min(dx)) * 2 -1 y = (y - min(y)) / (max(y) - min(y)) axs[1, 1].plot(dx, y, &#39;o&#39;) axs[1, 1].set_title(&#39;Diabetes dataset&#39;) for ax in axs.flat: ax.label_outer() . . Interpolation vs fit . The goal is to recover the ground truth, given the samples. As can be seen in the examples this is possible for the first few examples, but for the diabetes example there are multiple values for the same $x$. Let&#39;s make it concrete what we&#39;re trying to do. . We are given points $x_1,...,x_n$ and values $y_1,...,y_n$ and we want to select a polynomial function $p(x)$ of degree $k$: . $$ p_k(x) = c_0 x^0 + c_1 x^1 + ... + c_k x^k $$ . For interpolation we want that $p(x_i)=y_i$ for $i=1,...n$. . For approximation we want to find a polynomial that approximates the value. For this we take the $L_2$ or squared difference distance: . $$ min_{p_k(x)} sum_i ( y_i - p_k(x_i) )^2 = min_{p_k(x)} || y_i - p_k(x_i) ||_2 $$ . Interpolation of a polynomial with monomials . For $n$ points there exists a unique polynomial of degree $n-1$ that passes through these points. To find this polynomial we can solve a linear system of $n$ equations with $n$ variables. . Example for n=2 . As an example take the points $(-0.5,0.25), (0, 0), (0.5, 0.25)$ and we want to find a quadratic polynomial $p(x) = c_0 + c_1 cdot x + c_2 cdot x^2$. . The linear equations will be $$p(-0.5) = 0.25 = c_0 + c_1 cdot -0.5 + c_2 cdot (-0.5)^2 $$ $$p(0) = 0 = c_0 + c_1 cdot 0. + c_2 cdot 0^2 $$ $$p(0.5) = 0.25 = c_0 + c_1 cdot 0.5 + c_2 cdot (0.5)^2 $$ . For this example we can solve by hand by noting that $c_0=0$ (second equation), which leaves $$ 0.25 = -c_1/2 + c_2/4 $$ $$ 0.25 = c_1/2 + c_2/4 $$ . and thus $c_1=0$ and $c_2=1$. . This is a good thing since I picked the points from the function $f(x) = x^2$ :). . Matrix notation . It is convenient to write the equations above in matrix form and make the coefficients $c$ explicit. In the optimization we keep degree $k$ fixed, but have to pick the best $c$. . We can do this in matrix form and state $A_k(x)c=y$, where $A_k(x)$ is the matrix for the values $x$ and the powers up to $k$ (the monomials): . For $k=2$ it would be . $$A_2(x) = begin{bmatrix} 1 &amp; -.5 &amp; .25 1 &amp; 0 &amp; 0 1 &amp; .5 &amp; .25 end{bmatrix} $$ For approximation we would write . $$ min_c ||y - A_k(x)c||_2 $$ . The matrix of the monomials is called the Vandermonde matrix. Using the functionality in NumPy gives the same matrix as above (but for two extra points): . np.vander(x, 3, increasing=True) . array([[ 1. , -1. , 1. ], [ 1. , -0.5 , 0.25], [ 1. , 0. , 0. ], [ 1. , 0.5 , 0.25], [ 1. , 1. , 1. ]]) . General . Equiped with matrix notation we solve $Ac=y$ for $c$. For example by taking the inverse $A^{-1}y=c$. . A = np.array([[1, -1/2, 1/4], [1, 0, 0], [1, 1/2, 1/4]]) y = np.array([0.25, 0., 0.25]) c = np.linalg.inv(A) @ y c . array([0., 0., 1.]) . We can do the same programmatically for the Runge function and degree $4$ . def interpolate(f, x): M = np.vander(x, increasing=True) c = np.linalg.inv(M) @ f(x) return c . . fig, axs = plot.subplots(2, 2) fig.set_size_inches(18.5, 10.5) f = lambda x: x**2 pf = np.polynomial.Polynomial(interpolate(f,x)) axs[0, 0].plot(x, f(x), &#39;o&#39;) axs[0, 0].plot(d, f(d), &#39;-&#39;) axs[0, 0].plot(d, pf(d), &#39;-&#39;) axs[0, 0].set_title(&#39;Quadratic&#39;) g = lambda x: 1/(1+(5*x)**2) pg = np.polynomial.Polynomial(interpolate(g,x)) axs[1, 0].plot(x, g(x), &#39;o&#39;) axs[1, 0].plot(d, g(d), &#39;-&#39;) axs[1, 0].plot(d, pg(d), &#39;-&#39;) axs[1, 0].set_title(&#39;Runge function&#39;) h = lambda x: x**14-x**3 ph = np.polynomial.Polynomial(interpolate(h,x)) axs[0, 1].plot(x, h(x), &#39;o&#39;) axs[0, 1].plot(d, h(d), &#39;-&#39;) axs[0, 1].plot(d, ph(d), &#39;-&#39;) axs[0, 1].set_title(&#39;Recliner&#39;) for ax in axs.flat: ax.label_outer() . . Approximation . What if we wanted to approximate with a lower degree polynomial than the number of points? For this we can use the Moore-Penrose inverse, a generalization of the normal matrix inverse $A A^{-1} = I$ . The Moore-Penrose inverse (Denoted as $A^+$) can be defined in terms of solving a linear system of equations $Ac=y$. The solution to $A^+y = hat{c}$ is the least squares solution i.e. $||Ac-y||_2 geq ||A hat{c}-y||_2, forall c$. . With this result we can approximate . def approximate(y, x, d=3): A = np.vander(x, N=d, increasing=True) c_approx = np.linalg.pinv(A) @ y return c_approx . A good question, that I do not know the answer too, is how the numerical instability (condition number) of the Moore-Penrose inversion relations to the regular inversion. If you know: please tell me. . Alternatively we can find the &#39;best&#39; solution to $Ac=y$ with np.linalg.lstsq. . def approximate(y, x, d=3): A = np.vander(x, N=d, increasing=True) c, _, _, _ = np.linalg.lstsq(A, y, rcond=None) return c . With degree 2: . degree = 3 fig, axs = plot.subplots(2, 2) fig.set_size_inches(18.5, 10.5) f = lambda x: x**2 pf = np.polynomial.Polynomial(approximate(f(x),x, d=degree)) axs[0, 0].plot(x, f(x), &#39;o&#39;) axs[0, 0].plot(d, f(d), &#39;-&#39;) axs[0, 0].plot(d, pf(d), &#39;-&#39;) axs[0, 0].set_title(&#39;Quadratic&#39;) g = lambda x: 1/(1+(5*x)**2) pg = np.polynomial.Polynomial(approximate(g(x),x, d=degree)) axs[1, 0].plot(x, g(x), &#39;o&#39;) axs[1, 0].plot(d, g(d), &#39;-&#39;) axs[1, 0].plot(d, pg(d), &#39;-&#39;) axs[1, 0].set_title(&#39;Runge function&#39;) h = lambda x: x**14-x**3 ph = np.polynomial.Polynomial(approximate(h(x),x, d=degree)) axs[0, 1].plot(x, h(x), &#39;o&#39;) axs[0, 1].plot(d, h(d), &#39;-&#39;) axs[0, 1].plot(d, ph(d), &#39;-&#39;) axs[0, 1].set_title(&#39;Recliner&#39;) DX, y = load_diabetes(return_X_y=True) dx = DX[:,2] # Map to [-1,1] dx = (dx - min(dx)) / (max(dx) - min(dx)) * 2 -1 y = (y - min(y)) / (max(y) - min(y)) pd = np.polynomial.Polynomial(approximate(y,dx,d=degree)) axs[1, 1].plot(dx, y, &#39;o&#39;) axs[1, 1].plot(d, pd(d), &#39;-&#39;) axs[1, 1].set_title(&#39;Diabetes dataset&#39;) for ax in axs.flat: ax.label_outer() . . With degree 14: . degree = 15 fig, axs = plot.subplots(2, 2) fig.set_size_inches(18.5, 10.5) f = lambda x: x**2 pf = np.polynomial.Polynomial(approximate(f(x),x, d=degree)) axs[0, 0].plot(x, f(x), &#39;o&#39;) axs[0, 0].plot(d, f(d), &#39;-&#39;) axs[0, 0].plot(d, pf(d), &#39;-&#39;) axs[0, 0].set_title(&#39;Quadratic&#39;) g = lambda x: 1/(1+(5*x)**2) pg = np.polynomial.Polynomial(approximate(g(x),x, d=degree)) axs[1, 0].plot(x, g(x), &#39;o&#39;) axs[1, 0].plot(d, g(d), &#39;-&#39;) axs[1, 0].plot(d, pg(d), &#39;-&#39;) axs[1, 0].set_title(&#39;Runge function&#39;) h = lambda x: x**14-x**3 ph = np.polynomial.Polynomial(approximate(h(x),x, d=degree)) axs[0, 1].plot(x, h(x), &#39;o&#39;) axs[0, 1].plot(d, h(d), &#39;-&#39;) axs[0, 1].plot(d, ph(d), &#39;-&#39;) axs[0, 1].set_title(&#39;Recliner&#39;) DX, y = load_diabetes(return_X_y=True) dx = DX[:,2] # Map to [-1,1] dx = (dx - min(dx)) / (max(dx) - min(dx)) * 2 -1 y = (y - min(y)) / (max(y) - min(y)) pd = np.polynomial.Polynomial(approximate(y,dx,d=degree)) axs[1, 1].plot(dx, y, &#39;o&#39;) axs[1, 1].plot(d, pd(d), &#39;-&#39;) axs[1, 1].set_title(&#39;Diabetes dataset&#39;) for ax in axs.flat: ax.label_outer() . . Note the beautiful overfitting on the diabetest dataset and how bad the Runge function is approximated. This post will be following by some other posts about: . Interpolation with Lagrange polynomials and derivatives in part 2 | Chebyshev nodes to avoid oscillation in part 3 | Solve $ geq$ invert in this article | . Hope you enjoyed it thusfar. .",
            "url": "vanderzwaan.dev/2022/02/05/Fit-polynomials-1.html",
            "relUrl": "/2022/02/05/Fit-polynomials-1.html",
            "date": " • Feb 5, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "PyPlot and Pylot",
            "content": "import matplotlib.pyplot as plt from sklearn import linear_model import numpy as np from matplotlib.collections import LineCollection from matplotlib.colors import ListedColormap, BoundaryNorm . The data . x = [[4*60+22], [4*60+6], [3*60+21], [5*60+25], [3*60+8]] y = [2_762_823, 6_184_448, 4_826_424, 3_544_263, 1_249_864] . Linear regression . reg = linear_model.LinearRegression() reg.fit(x, y) . LinearRegression() . p = np.array([[p] for p in np.linspace(3*60, 6*60, 20)]) yhat = reg.predict(p) . fig, ax = plt.subplots() # Scatter plt.scatter(x, y, c=y, cmap=&#39;plasma&#39;) plt.plot(p, yhat, color=&quot;pink&quot;, linewidth=3) plt.colorbar() plt.ylabel(&#39;#Streams&#39;) plt.xlabel(&#39;Duration (s)&#39;) plt.show() .",
            "url": "vanderzwaan.dev/2022/01/03/PyPlot-and-Pylot.html",
            "relUrl": "/2022/01/03/PyPlot-and-Pylot.html",
            "date": " • Jan 3, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "You shouldn't learn data science",
            "content": "Reading (or hearing) “I want to learn data science” is common. Followed by helpful comments and directions on how to do this and break into the profession. Somehow it makes sense. Both the ambition and the advice. But, does it make sense? . Consider similar questions “I want to learn mathematics” or “I want to learn consultancy”. . Most people would say that mathematics is a broad category and inquire about current knowledge. If the current knowledge is below a threshold perhaps some basic training can be recommended. Otherwise, it’s too broad and we have to dig deeper. Secondly, people would instantly ask “What for?”. What is the person actually trying to accomplish?1 . Consultancy is a very broad profession and/or job title. You can consult on basically anything, and therefore it’s impossible to answer the question. Learning how to handle clients is…well tough to answer on paper. Doing it is probably the best solution. . Let’s return to data science. Somehow there is a plan. There are tutorials. There are success stories of breaking into the industry. Consider the amount of jobs that were relabeled as data scientist. Seriously, can we answer such a question? . No we cannot. . Typically, this is the spot where the writer offers their wisdom and an alternative. Sorry. Not today. . If you’re an aspiring data scientist I now officially welcome you in the company of all other professions where you have to make the plan, do the work and be a bit lucky. . Changing profession or careers is difficult, and the challenges are different for each person. All I can offer is to make small but steady improvements towards your goal. . Funnily the road towards being an academic mathematician is simple. Executing this is hard (master, phd, tenure, …). &#8617; . |",
            "url": "vanderzwaan.dev/thoughts/2021/11/09/learning.html",
            "relUrl": "/thoughts/2021/11/09/learning.html",
            "date": " • Nov 9, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Better QR codes",
            "content": "QR codes are quite interesting, but boring to look at. For this reason I introduce SR codes, which stands for Snake Race codes. It improves QR codes on several fronts: . Animation! | Color! | Hardcore. QR codes have error correction. SR codes don&#39;t. If you make an error it&#39;s game over. | Not quick. Savour the wait before you can scan the code. | . Developing a scanner app is left as an exercise for the reader :) . . QR codes are really interesting. Error correction and packing as much information as you can in a small amount of pixels while keeping it robust is a nice challenge. . Nice explanation in Dutch https://michielp1807.github.io/qr-codes/ or check out the Wikipedia page. . Representing position and orientation . We represent the position of the snake as a point $(x, y)$ with an orientation $(h, v)$. Taking turns is done with a rotation matrix that defines 90 degree turns and going straight (=not changing direction). . actions = [ # Left np.array([ [0, 1], [-1, 0] ]), # Straight np.array([ [1, 0], [0, 1] ]), # Right np.array([ [0, -1], [1, 0] ]) ] . For example, if the snake is going to the right it has orientation $(1,0)$. If it makes a 90 degree turn to the left it goes up and has orientation $(0, -1)$. The first coordinate is horizontal and the second is vertical. . actions[0] @ np.array([1, 0]) . array([ 0, -1]) . Going left four times we should end up in the same direction: . orientation = np.array([1, 0]) for i in range(4): orientation = actions[0] @ orientation orientation . array([1, 0]) . Going places . We want to see where the snakes have been as well as their current position. Below we define: . The different snakes in snakes, including start position, color and trail | For each snake we also keep track if they are active. The snake stops if there is no valid move to make. | To make our lives easy we have per position the information if some snake has been there to limit possible moves matrix | . Define a method to generat the trails of the snakes . def generate_trails(size: int, bits): # Define quarter distance q = size // 4 # Define the start positions of all snakes snakes = [ { &#39;c&#39;: &#39;Greens&#39;, &#39;position&#39;: np.array([q+1, q+1]), &#39;orientation&#39;: np.array([1, 0]), &#39;trail&#39;: np.full((2*(size+2), 2*(size+2)), 0), &#39;active&#39;: True }, { &#39;c&#39;: &#39;Reds&#39;, &#39;position&#39;: np.array([3*q+1, 1+1]), &#39;orientation&#39;: np.array([0, 1]), &#39;trail&#39;: np.full((2*(size+2), 2*(size+2)), 0), &#39;active&#39;: True }, { &#39;c&#39;: &#39;Blues&#39;, &#39;position&#39;: np.array([3*q+1, 3*q+1]), &#39;orientation&#39;: np.array([-1, 0]), &#39;trail&#39;: np.full((2*(size+2), 2*(size+2)), 0), &#39;active&#39;: True }, { &#39;c&#39;: &#39;Purples&#39;, &#39;position&#39;: np.array([q+1, 3*q+1]), &#39;orientation&#39;: np.array([0, -1]), &#39;trail&#39;: np.full((2*(size+2), 2*(size+2)), 0), &#39;active&#39;: True } ] # Keep track of where the snakes have passed matrix = np.full((size+2, size+2), False) # Set the borders matrix[0, :] = True matrix[:, 0] = True matrix[-1, :] = True matrix[:, -1] = True # Go over the bits current_bit = 0 iteration = 0 # While there are active snakes try it out while any([snake[&#39;active&#39;] for snake in snakes]): # For each active snake active_snakes = [snake for snake in snakes if snake[&#39;active&#39;]] for i, snake in enumerate(active_snakes) : # Get the bit or exit if current_bit &gt;= len(bits): return snakes bit = bits[current_bit] # Get the possible actions possible_actions = [ action for action in actions if not matrix[ tuple(snake[&#39;position&#39;] + action @ snake[&#39;orientation&#39;]) ] ] # Execute an action if len(possible_actions) == 0: snake[&#39;active&#39;] = False continue # See if other snakes can still play elif len(possible_actions) == 1: action = possible_actions[0] elif len(possible_actions) == 2: action = possible_actions[int(bit)] elif len(possible_actions) == 3: offset = iteration % 2 action = possible_actions[(int(bit) + offset)] # Update current_bit += 1 snake[&#39;orientation&#39;] = action @ snake[&#39;orientation&#39;] prev_position = snake[&#39;position&#39;] snake[&#39;position&#39;] = snake[&#39;position&#39;] + snake[&#39;orientation&#39;] # Update boundaries where snakes have been matrix[ tuple(snake[&#39;position&#39;]) ] = True # Update the twice larger pretty image (so that there is space and it looks pretty) snake[&#39;trail&#39;][ tuple(prev_position + snake[&#39;position&#39;]) ] = 2 * iteration snake[&#39;trail&#39;][tuple(2 * snake[&#39;position&#39;]) ] = 2 * iteration + 1 # Update iteration iteration += 1 return snakes, current_bit, iteration . Showing it on screen . Some code that generates the image at a certain frame/point in time: . def generate_image(snakes, frame=np.inf, canvas_size = 44): # Define a transparant layer transparent = np.uint8(np.broadcast_to( np.array([255, 255, 255, 0]), (canvas_size, canvas_size, 4) )) # Define the bottom layer (transparent white) composite = Image.new( &#39;RGBA&#39;, (canvas_size, canvas_size), (255, 255, 255, 255) ) # Draw each trail for snake in snakes: # Get the trail until this frame trail = snake[&#39;trail&#39;] * (snake[&#39;trail&#39;] &lt;= frame) # Get the color color = cm.get_cmap(snake[&#39;c&#39;]) # Set the trail and rest transparent df = np.where( trail[:, :, np.newaxis] &gt; 0, np.uint8(color(trail / trail.max()) *255), transparent ) # Create an image and add the layer im = Image.fromarray(df) composite = Image.alpha_composite(composite, im) d = ImageDraw.Draw(composite) d.rectangle( [(0, 0), (canvas_size-1, canvas_size-1)], fill=None, outline=(0,0,0,255), width=1) return composite.resize((5 * canvas_size, 5 * canvas_size), Image.NEAREST) . For this piece of code we&#39;ll generate 500 bits on a canvas of 22x22. The bits are not generated efficiently. . seed = 0 nr_bits = 500 size = 22 # Sizes of the canvas canvas_size=2*(2+size) canvas_size_xl = canvas_size * 5 # Generate some random bits rng = default_rng(seed) bits = rng.random(nr_bits) &lt; 0.5 # Get the trails snakes, nr_generated, max_iteration = generate_trails( size, bits ) # Display image = generate_image(snakes, frame=max_iteration, canvas_size=canvas_size) display(image) . Animation . Finally we create a GIF image . gif = Image.new(&#39;RGBA&#39;, (canvas_size_xl, canvas_size_xl)) gif.save(&#39;animation.gif&#39;, save_all=True, append_images=[generate_image(snakes, frame=frame, canvas_size=canvas_size) for frame in range(2, max_iteration)], duration=100, loop=0) . .",
            "url": "vanderzwaan.dev/2021/10/23/SR-codes.html",
            "relUrl": "/2021/10/23/SR-codes.html",
            "date": " • Oct 23, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Popularity as a flawed proxy",
            "content": "Python is the most popular programming language for machine learning. So sayeth the internet… So you should pick Python to learn over other languages. But, is it? Should you? . Bla bla bla . Most of these articles have no basis and combine some opinions on how Python code is clean, it’s popular (notice the circular reference?) and that it has many libraries. Every update of an index (for example TIOBE) spawns derivative articles how Python went up a notch, or Julia is posed to overtake it. . I think it’s good to take a step back and ask yourself: . How did they measure popularity? | Is this representative for my field? | Do I actually care about popularity? | . The aforementioned TIOBE index takes the number of results in search engines as input. Seems reasonable, but more is not always better. In my bubble you sometimes think that everyone has a blog, but there is a big world of enterprise software. In this setting there is less blogging and tweeting, but there are great engineers. . Fundamentals, not popularity . Popularity might seem like a good proxy to choose a language to learn, but it’s hard to gauge the real popularity. There are many echo chambers/bubles in which the gospel of X is spread. . I argue that you should go fundamentals first. Yes, being fluid in a language helps, but being shitty at the basics results in shitty code. Enjoying the coding is also important, so if you enjoy Julia, R, Java or Swift more than Python: by all means, start with that. . The future . Many reasons that these articles mention why Python is great might be moot. Pretty soon we’ll get compiler infrastructure for GPU’s and all the hand-tuned libraries in Pytorch and Keras will be available for all languages that target LLVM (even Python does this through Numba). . Swift allows you to mix Python code into your Swift project, so you don’t even need to choose. . So pick Python because it’s fun, beautiful or convenient because a great course on ML is using it. Learn fundamentals. Then, also learn other languages a little to understand the differences. . CV . All advice above is moot for applying for jobs. For this I would actually advice to learn all popular tools and have sample projets. You know what the recruiter/HR will ask ;) .",
            "url": "vanderzwaan.dev/2020/07/31/popularity-as-a-flawed-proxy.html",
            "relUrl": "/2020/07/31/popularity-as-a-flawed-proxy.html",
            "date": " • Jul 31, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Python vs Javascript for neural networks",
            "content": "In this article I describe why I would prefer Python to JavaScript specifically for neural networks. . This post is only based on actual features of the language and its design, not hand waving of ‘popularity’. Both Python and JavaScript are general programming languages. Both can accomplish any task and probably in comparable speed. So let’s focus on the specific domain: neural networks. . Through some simple programs you can see why Python could be appealing. This is based on my observations to program auto differentiation in JavaScript. . The conclusions could be extended to other machine learning techniques. This is left as an exercise for the reader™. . The Premise . Programming languages bridge the gap between the real world and computers. Through the language we encode our ideas and hope it works as expected. If the code resembles how we think about a topic it makes it much easier. For example, anything can be made unintuitive if programmed in assembly. For many domains there are domain specific languages. Either full-blown programming languages or embedded. Haskell and Swift are particularly great at this. . This will be the biggest difference between Python and JavaScript. Python is quite amenable to this process, and JavaScript is not - for logical historical reasons, I might add. . Auto differentiation . Auto differentiation is a core component of neural networks and how to build them efficiently. To perform back propagation you need to (approximately) calculate the gradient. . The language of differentiation is functions and mathematical operations. For example $f(x)=(x+6)*7$. All operations in neural networks are basic mathematical operations, most on tensors. Let’s write code to represent a function in Python and Javascript. . Operator overloading . In both languages we want an object of a class Value that represents the function for a particular input. As a building block we need to write down the computations we want to do. Visually the Python code looks nicer. . Python: . x = Value(5) y = (x + 6) * 7 . In the code above y is actually a Value! Furthermore, we get the whole computation tree of intermediate values. . JavaScript: . let x = new Value(5) let y = mult(add(x, 6, 7)) . This is because Python allows us to extend arithmetic on objects with custom functions by defining __add__ on the object as follows: . class Value: def __init__(self, x, _deps=[]): self.data = x def __add__(self, other): other = other if isinstance(other, Value) else Value(other) return Value(self.data + other.data, _deps=[self, other]) def __mul__(self, other): other = other if isinstance(other, Value) else Value(other) return Value(self.data * other.data, _deps=[self, other]) def __radd__(self, other): return self + other def __rmul__(self, other): return self * other . Objects as functions . Every neuron in the neural network consists of weights, biases and an activation function: . neuron(input) = relu(weight * input + bias) . In most approaches to perform back-propagation its handy to also have some mutable state in the neuron. So it’s both a function and an object. . In Python we can use an object as a function which results in this code: . neuron = Neuron(5) output = neuron([1,2,3,4,5]) . For JavaScript we would need to . let neuron = Neuron(5) let output = neuron.eval([1,2,3,4,5]) . List comprehensions . List comprehensions and generators are two features of Python that I really miss in other languages. . In Python we can multiply two arrays A and B by doing: . result = [a*b for a,b in zip(A, B)] . In JavaScript you would need to do implement the zip function and do: . result = zip(A, B).forEach(t =&gt; t[0]*t[1]) . However, the zip function actively constructs the array of tuples instead of lazily generating it during the iteration. JavaScript actually supports generators but it requires more coding, because there is no natural way to map over them: . function* zip(A, B) { let i = 0; while(i &lt; Math.min(A.length, B.length) ) { yield [A[i], B[i]]; i++; } return [A[i], B[i]]; } let result = []; for(let t of zip(A, B)) { result.push(t[0] * t[1]) } . Wrap-up . Obviously you can build neural networks in JavaScript. I hope that through these examples you see how much closer to reality you can get in Python and why people would prefer it. . However, there is a small downside to the Python code: to a beginner it appears as magical and it’s harder to find the place where the code is defined. For JavaScript it’s explicit. .",
            "url": "vanderzwaan.dev/2020/07/13/python-vs-javascript.html",
            "relUrl": "/2020/07/13/python-vs-javascript.html",
            "date": " • Jul 13, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Scaffolding in Eleventy",
            "content": "I’ve rebuild my website with Elventy and Tailwind. One feature that I really liked about Hugo was to scaffold new posts, so I’ve build my own Steiger. . All of my blogs have been static sites that are generated in a build process and then pushed to some hosting. Each post is simply a markdown page in the folder denoting it’s date, so posts/2020/07/06/scaffolding-in-eleventy.md for this particular post. Each page also has metadata for the title, description, etc… . title: &quot;Scaffolding in Eleventy&quot; description: &quot;Scaffolding in Eleventy&quot; date: 2020-07-06T16:27:46.409+02:00 draft: false tags: [Elventy, static website] . With these things I’m really lazy. I do not want to copy/paste, create folders, etc. Just execute scaffold posts/2020/07/06/scaffolding-in-eleventy.md and I want to have a premade template where I fill in the content. . Unfortunately it was not included in Eleventy, so I rolled my own using Chalk for colored terminal output, Commander for the CLI, Luxon for dates, times (because the vanilla dates in Javascript breaks your brain), Nunjucks for templating. . The package has reached version 0.0.2 and it autogenerates the file you want, checks if it exists and gives the current datetime to the template. Over time as I use it, I’ll probably add more features. .",
            "url": "vanderzwaan.dev/2020/07/06/scaffolding-in-eleventy.html",
            "relUrl": "/2020/07/06/scaffolding-in-eleventy.html",
            "date": " • Jul 6, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Building Autodiff from Scratch",
            "content": "I’ve made an interactive notebook with Observable to show how you can implement autodifferentiation from scratch. .",
            "url": "vanderzwaan.dev/2020/07/06/building-autodiff-from-scratch.html",
            "relUrl": "/2020/07/06/building-autodiff-from-scratch.html",
            "date": " • Jul 6, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Inject Context",
            "content": "This is a small post on how to inject services/contexts into functions. . The motivating example is working with databases and sessions in a webservice. Each call you create a database session, do your stuff, and close the session. When everything goes right, commit. On errors it need to rollback. . Luckily Python has a nice feature called context managers that you can use with a with statement: . # POST /cats/{cat_id:int} def route(request): # Get the parameters and information from the form description = request.form().get(&#39;description&#39;, None) cat_id = request.path_params[&#39;cat_id&#39;] # Open a session and query the database with db.session() as session: cat = session.query(model.Cats).filter(model.Cats.id == cat_id).first() if cat is None or description is None: raise Exception(&quot;Need more information&quot;) cat.description = description session.commit() # An ORM like SQLAlchemy automatically flushes the changes of cat to the database . This gets tedious when a lot or your routes have this piece of code. Some web frameworks allow ‘middleware’ and you could inject the session there…but now every route opens a session with your database: that seems overkill. (But hopefully you or your library uses a connection pool). . Premature optimization is the root of fun and evil, so it’ll be interesting to see how the code looks like. . Our goal will be to get the following form: . # POST /cats/{cat_id:int} def route(request, db_session): description = request.form().get(&#39;description&#39;, None) cat_id = request.path_params[&#39;cat_id&#39;] cat = db_session.query(model.Cats).filter(model.Cats.id == cat_id).first() if description is None or cat is None: throw Exception(&quot;Need more info!&quot;) cat.description = description . We introduce a Depends class that wraps around a context manager and is declared as default parameter. In Python default parameter values are evaluated when the function is defined, so the Depends wrapper will later give us the session context manager. To change the function we’ll use a decorator automagic that will inject this new functionality. . # POST /cats/{cat_id:int} @automagic def route(request: Request, db_session: DBSession = Depends(db.session()), ): description = request.form().get(&#39;description&#39;, None) cat_id = request.path_params[&#39;cat_id&#39;] cat = db_session.query(model.Cats).filter(model.Cats.id == cat_id).first() if description is None or cat is None: throw Exception(&quot;Need more info!&quot;) cat.description = description . Alternative . The alternative is to make a simpler decorator that injects the session in a pre-determined keyword like this: . @inject_db_session def route(request: Request, db_session: DBSession), ): description = request.form().get(&#39;description&#39;, None) cat_id = request.path_params[&#39;cat_id&#39;] cat = db_session.query(model.Cats).filter(model.Cats.id == cat_id).first() if description is None or cat is None: throw Exception(&quot;Need more info!&quot;) cat.description = description . The decorator is much easier to write: . from functools import wraps def inject_db_session(func): @wraps(func) def wrapped(*args, **kwargs): return func(*args, **{**kwargs, db_session}) return wrapped . It has several features of interest though: . It uses functools.wraps. This ensures that the name and the documentation of the wrapped function stay inplace. See this excellent Stackoverflow answer. | It combines the named arguments **kwargs with the new keyword in such a way that the new db_session supersedes the case when the function would be called with the same keyword. | . The code . Below you can see the code that I wrote for this and it works with an arbitrary number of Depends. . The followup question is of course: . Can we also get rid of the boilerplate for getting a path parameter and formdata? For a request endpoint most of the information should come from path parameters and the form (i.e. the body of the request). . Here we have the following assumptions: . We will use Pydantic to parse the formdata. | Depends parameters are replaced with their context managers; | Simple (int, str, …) arguments are replace by path parameters according to name; | Pydantic model arguments are used to parse the form data. Errors are handled by the automagic. You can have at most one of these models. | . It’ll look like this: . # POST /cats/{cat_id:int} @automagic def route(request: Request, cat_id: int, catUpdateRequest: CatUpdateRequest, db_session: DBSession = Depends(db.session()), ): cat = db_session.query(model.Cats).filter(model.Cats.id == cat_id).first() if cat is None: throw HTTPException(404, &quot;Give a valid cat id&quot;) cat.description = catUpdateRequest.description . This is also the route that FastAPI took to create their routes. However, I don’t know about the implementation. Funnily I found this framework because I wanted the above functionality, made a prototype and Googled it later. . Let’s first define some mock objects: . from typing import Callable, List from contextlib import contextmanager, ExitStack import inspect import pydantic from functools import wraps class DBSession: &quot;&quot;&quot; A mock DBSession class that has the most important functions of a session with a database. &quot;&quot;&quot; def query(self, needle, haystack): return f&quot;Finding {needle} in {haystack}: {needle in haystack}&quot; def commit(self): pass def rollback(self): pass def close(self): pass class Database: &quot;&quot;&quot; A mock Database class that can create a session scope (this is similar to how it would work with SQLAlchemy). &quot;&quot;&quot; def __init__(self, url): # Do some initialization self.url = url def connect(self): pass def disconnect(self): pass def create_session(self): return DBSession() @contextmanager def session_scope(self): session = self.create_session() try: print(f&quot;Create session for database {self.url}&quot;) yield session session.commit() except: session.rollback() raise finally: print(f&quot;Close session for database {self.url}&quot;) session.close() . Now we write our Depends wrapper: . class Depends: &quot;&quot;&quot; Small wrapper to be able to determine which resources &quot;&quot;&quot; def __init__(self, c: Callable): self.c = c def __call__(self): return self.c . Now the good stuff, the automagic function: . def automagic(f): # Get the signature and extract dependencies and the formdata signature = inspect.signature(f) dependencies = {n: p.default for n, p in signature.parameters.items() if p.default.__class__ == Depends} form_name, form_class = next(((n, p.annotation) for n, p in signature.parameters.items() if issubclass(p.annotation, pydantic.BaseModel)), None) # Create the wrapped function, construct the context managers and inject the information @wraps(f) def wrapped(*args, **kwargs): form = {} if form_name: form = {form_name: form_class(**request[&#39;form_data&#39;])} with ExitStack() as stack: resources = {} for name, resource in dependencies.items(): res = stack.enter_context(resource()) resources[name] = res f(*args, **{**kwargs, **request[&#39;path_params&#39;], **resources, **form}) return wrapped . Finally using all these goodies: . db1 = Database(&quot;localhost:666/beast&quot;) db2 = Database(&quot;localhost:42/answers&quot;) class Form(pydantic.BaseModel): answer: int answers: List[int] @automagic def sample_route( request, neighbour: int, form: Form, s1=Depends(db1.session_scope()), s2=Depends(db2.session_scope())): print(f&quot;Query: {s1.query(neighbour, [665, 667])}&quot;) print(f&quot;Query: {s2.query(form.answer, [42])}&quot;) print(&quot;I know all the answers now!&quot;) request = { &#39;path_params&#39;: { &#39;neighbour&#39;: 667 }, &#39;form_data&#39;: { &#39;answer&#39;: 43, &#39;answers&#39;: [&#39;42&#39;] } } sample_route(request) .",
            "url": "vanderzwaan.dev/2020/03/17/inject-context.html",
            "relUrl": "/2020/03/17/inject-context.html",
            "date": " • Mar 17, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Clojure",
            "content": "Through the labyrinth of the internet I somehow ended up watching Rich Hickey, the creator of Clojure. Now and then the stars align and I navigate Youtube, which I found to be badly sorted and organized. . It’s a great talk and it made me want to try out Clojure. My goal was to learn Swift, but, …, procrastination happened? I did a dozen or so exercises on Hackerrank. . One of the exercises was to reverse a list, without using the built-in function. Here it started to be weird. . My first thought was to use the reduce function where you walk through the list and built up a new result. conj[oin] is used to return a new collection with the element added. . Let’s skip reading the documentation completely of course :) . Doesn’t work: . (fn [list] (reduce conj [] lst) ) . Works: . (fn [list] (reduce conj () lst) ) . There is a difference based on the type () is a list, [] is a vector. And, as is stated clearly in the documentation conj works differently for each datatype: . For lists it adds at the end, the underlying implementation is a singly-linked list. So, makes sense. | For vectors it adds at the beginning | . Probably the best option is to use sequences and use cons. But how that would exactly work…a quick try in the editor of Hackerrank just gave me stack traces that my function definition was wrong. Copy-pasting an example from the Clojure docs and the problem persisted. .",
            "url": "vanderzwaan.dev/2020/02/07/watching-rick-hickey-and-trying-out-clojure.html",
            "relUrl": "/2020/02/07/watching-rick-hickey-and-trying-out-clojure.html",
            "date": " • Feb 7, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Back to Hugo ",
            "content": "Last year I experimented with Sapper Sapper to create a static blog, but I’ve already switched back to Hugo. Mainly because I’ve done the experiment and learned from it. . Writing a post had little speedbumps and the result was no posts since october. At least I know where the line is for me. .",
            "url": "vanderzwaan.dev/2020/01/09/back-to-hugo.html",
            "relUrl": "/2020/01/09/back-to-hugo.html",
            "date": " • Jan 9, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Import fun with Parcel",
            "content": "Today I was having fun with Parcel and generating a bundle for NodeJS. Very convenient, only importing seemed weird. Some imports were ‘successful’ in the sense that both Parcel and NodeJS could resolve it, but the import was {default: {}}. Hmmm. . I also write my thoughts on bundlers I used. Depending on your standards it might or might not be a review. . Bundling . A JavaScript bundler is a tool that puts your code and all its dependencies together in one JavaScript file. . There are many reasons you want this. One of the main reasons is if you develop a client side Javascript library. You actually need to serve all your Javascript and cannot rely on the client to have dependencies installed. . Further, most of these bundlers transpile sane Javascript to insane Javascript that is backwards compatible, and minify it. Notice that sane is relative here. . If you’re stumped by these bundlers and why it seems a big deal, you’re probably programming in Go or Java where the compiler actually makes bundles for you. . For NodeJS backend applications you could rely on the package information and get the dependencies when deploying or include them in your Docker image. But I do not like that. . Even if you’re meticulous in pinning exact versions of your dependencies other could forget to update version numbers. Or, pull their packages from NPM and all your deployments/builds fail. . You argue that I should not be so cynical. Ok, I hope you at least stop pulling dependencies during deployment, but I’ll go with it. . It’s also wasteful. You’re pulling huge amounts of dependencies and most likely you’re only using a small fraction. For me it’s no exception to have 50-100mb of modules but the compiled Javascript file is ~20kb. You can make deployment much faster and it’s self-contained. . Bundlers . I’ve used webpack, Rollup and Parcel. Webpack feels like making art with spaghetti and glue. It’s about the process, not the result and only fun if you’re not cleaning it up. I’m not biased against spaghetti and glue, but rather not have them in my code. . Rollup was pretty nice to use and I started using it because of Sapper and Svelte. Support for NodeJS bundles is lacking and Node Addons are not supported. I tried to make my own plugin for this but…one hour of weird errors and I bailed. Also because I seemed to include many plugins to make it work for NodeJS. . Parcel was working within 5 minutes and got me a nice NodeJS bundle, including the binaries in a Node Addon. Add a nice code generator and we’re good to go! Or not. One weird error got my stumped. . Consider two imports of files with special characters: . import * as route_1 from (&#39;./routes/a/b/c/[slug].js&#39;) import * as route_2 from (&#39;./routes/a/b/c/&lt;slug&gt;.js&#39;) . The imports resolve and, yes, both files really, really, really exist. . Normally this works fine, but Parcel does something with it. The imports resulted in: . route_1 = {default: {}} route_2 = {get: [AsyncFunction: get], post: [AsyncFunction: post] } } . Huh. . I did find out what the underlying issue is (probably some regex stuff in imports), but even when escaping the square brackets it did not work. The point brackets worked, so I consider it solved. .",
            "url": "vanderzwaan.dev/2019/10/28/import-fun-with-parcel.html",
            "relUrl": "/2019/10/28/import-fun-with-parcel.html",
            "date": " • Oct 28, 2019"
        }
        
    
  
    
        ,"post18": {
            "title": "Experiment: Blog with Sapper",
            "content": "This blog now runs on Sapper. The experiment succeeded but I do miss some quality of life functionality of Hugo. . Why change? . This blog used to be a very simple static website that was generated by Hugo from some Markdown files. It’s very convenient to work and I automatically deployed new versions whenever I pushed changes to the repository. . Now, if life was easy why would I change? Because playing with Sapper is fun, specifically building a front end with the underlying framework Svelte is lots of fun. . Svelte and Sapper . Svelte is in the same category as React and Vue. It’s a framework to build a front end and stay sane. Building an interactive website with vanilla Javascript usually makes me want to cry. . I’ve worked with React and Vue before and although it was quite smooth and the documentation is perfect, I always felt I type lots of code but it doesn’t do much. Both frameworks were getting in the way and it felt like magic sprinkled with other peoples components. Ugh. Getting an autocomplete to work where I needed not solution x but x’ was a horrible few hours. . Svelte felt different. I could write code as I wanted to and Svelte would compile vanilla Javascript to Javascript that interacted with the DOM. It feels great! As a test I wrote a pixel editor that could import files and drag and drop…from scratch. It felt wonderful and very productive. . So far my love story with Svelte. . Sapper is like Next.js but then with Svelte. Normally I build lots of stuff in Python but this tempted me to build applications with Sapper. . I still have to get used to NodeJS libraries and NPM compared to Python. Packages for Python feel complete and robust. In Javascript land…ugh…people really stack lots and lots of libraries on top of each other. It cannot be good if there is a package left-pad that puts the Javascript world in chaos as it’s pulled from NPM. . But Sapper tempts me. . Sapper export . Sapper has a command to export the application as a static website. It does so by sniffing out the calls to the server side and links and storing the results. . Seems pretty good, but there is no official support for Markdown, so you need to do some work yourself. I used the official website, and two repositories as a starting point to glean how they made it work: David’s blog and from a repository of Maxi Ferreira. . The main idea is to find .md files and put those in a data structure so that a webservice can serve the JSON (with raw HTML for the .md documents) to the client side application. . In the examples the .md files were in a single directory and they didn’t use images. Currently images should be in the static folder. The workflow is then to first upload images, and then refer to them from your .md file. Awkward. . Goal . The goal is to support the following directory structure: . /posts/2019/01/09/dogs.md /posts/2019/11/10/fishes/index.md /posts/2019/11/10/fishes/happy-fish.png /posts/2019/11/10/fishes/sad-fish.png . The /posts/2019/11/10/fishes/index.md document refers to happy-fish.png and sad-fish.png with a relative path. Your favorite editor can then give nice previews. . I couldn’t make it work with Sapper to have an arbitrary amount of /’s in the URL, so I made due with - instead. . The slugs in the URL should be: . vanderzwaan.dev/blog/2019-01-09-dogs vanderzwaan.dev/blog/2019-11-10-fishes . The images should be reachable with urls: . 2019-11-10-fishes/happy-fish.png 2019-11-10-fishes/sad-fish.png . Find all Markdown files . Finding the .md files should be easy, something like this will do: . const locations = glob.sync(`${POSTS_DIR}/**/*.md`); export const posts = locations.map( loc =&gt; getPost(loc)).map(post =&gt; parsePost(post)); . There was a bit of logic to map file names to slugs: a/b/c/index.md to a-b-c and a/doc.md to a-doc. Nothing terribly complicated. . Create an endpoint /blog/[slug]/[img] . We need to be able to serve the images in the blogpost so let’s create an endpoint that returns a certain image for a post. But then we also need to have a list of images and their locations. . For each post we also find the images in the same folder and store them: . const attachments = new Map(); glob.sync(`${base}/*.png`) .map(f =&gt; getFileInformation(f, POSTS_DIR)) .forEach(attachment =&gt; { attachments.set(attachment.filename, attachment); }); . The service is a standard Express server route that serves an image. . Add images to sapper export . It seems as if we should be done, but I found out that sapper export only follows links and not the references. . This required some code changes to Sapper export to also follow href attributes and a new mode to read files as images are in binary. . I should probably make a pull request for this, but currently the code is hacky. . #Result . I’m pretty happy with the result, but I do miss the following nice things from Hugo: . Scaffolding | It watches the .md files, Sapper does not (so manual recompile) | .",
            "url": "vanderzwaan.dev/2019/10/14/experiment-blog-with-sapper.html",
            "relUrl": "/2019/10/14/experiment-blog-with-sapper.html",
            "date": " • Oct 14, 2019"
        }
        
    
  
    
        ,"post19": {
            "title": "Starlette: A tale that was silently swallowed by uvicorns.",
            "content": "Imagine the following bugreport of your Starlette application: . Calling your API results 30% of the time in an error and 70% in correct behaviour. . and you know that these calls are time independent and should either always work or never work. Locally you cannot reproduce it, but the error can be traced to a datastructure being empty…which is filled during startup. . This is the offending code: . @app.on_event(&#39;startup&#39;) async def startup(): logger.info(&quot;Executing the startup phase...&quot;) create_datastructure() logger.info(&#39;Ready to go!&#39;) . Locally you can never let create_datastructure() fail and it doesn’t make any external calls (it reads a local file and processes it). . Going deeper: in production there were 4 workers and locally only 1, and 30% is suspiciously close to a quarter. . Turns out that errors during a startup hook are silently swallowed by Uvicorn and the worker will still start. Add to that unexplicable behaviours of AWS Elastic Beanstalk to randomly insert chaos during startup. . The annoying part is that AWS Elastic Beanstalk is very slow and the deployment (in my experience) is frail and prone to fail. It all takes quite a while and you need to do yet another deployment… . So, this is the new code to at least detect that AWS Elastic Beanstalk causes some error that Uvicorn than kindly sweeps under the rug. Now, it’ll fail, as it should. . @app.on_event(&#39;startup&#39;) async def startup(): try: logger.info(&quot;Executing the startup phase...&quot;) create_datastructure() logger.info(&#39;Ready to go!&#39;) except Exception as e: logger.error(str(e)) raise e .",
            "url": "vanderzwaan.dev/2019/08/13/starlette-a-tale-that-was-silently-swallowed-by-unicorns.html",
            "relUrl": "/2019/08/13/starlette-a-tale-that-was-silently-swallowed-by-unicorns.html",
            "date": " • Aug 13, 2019"
        }
        
    
  
    
        ,"post20": {
            "title": "Starlette and Socketio Improvements",
            "content": "This is a followup of the previous article on using Starlette together with Python-SocketIO and background processes. . I’ve made some improvements, especially to handle SIGINT (ctrl+c) properly. Additionally, I wanted to start background processes without waiting for a client to connect (this is a solution I saw online). . The core problem was that all async tasks should be on the same loop. The solution took some digging around uvicorn internals, but the following worked: . Get the Uvicorn server as an awaitable; | Join the awaitable with your background tasks and return when the first completes | Run the composed application. | . This properly responds to SIGINT and all processes start without waiting for outside interaction. . Background tasks are more like workers here. One-off tasks fall in two categories: . Startup or shutdown: Use Starlette hooks for startup and shutdown to handle work there; | Based on interactions from outside: Just call tasks when an API is called. | . import logging import uvicorn from app import app import asyncio from uvicorn.loops.uvloop import uvloop_setup logging.basicConfig( level=logging.INFO, format=&quot;%(asctime)-15s %(levelname)-8s %(message)s&quot; ) def uvicorn_task(): &quot;&quot;&quot; Returns running the app in uvicorn as an awaitable to join the main asyncio loop. &quot;&quot;&quot; config = uvicorn.Config(app, host=&#39;0.0.0.0&#39;, port=8000) server = uvicorn.Server(config) return server.serve() async def main(app): &quot;&quot;&quot; Joins unicorn together with background tasks defined in the app. &quot;&quot;&quot; # Make the list with awaitables aws = [uvicorn_task(), *app.background_tasks()] # Run and return when the first completes or is cancelled await asyncio.wait(aws, return_when=asyncio.FIRST_COMPLETED) if __name__ == &#39;__main__&#39;: # Set up the loop uvloop_setup() loop = asyncio.get_event_loop() # Run the main loop asyncio.run(main(app)) .",
            "url": "vanderzwaan.dev/2019/06/13/starlette-and-socketio-improvements.html",
            "relUrl": "/2019/06/13/starlette-and-socketio-improvements.html",
            "date": " • Jun 13, 2019"
        }
        
    
  
    
        ,"post21": {
            "title": "HTTP Redirects and Funny behaviour",
            "content": "Recently I had a weird problem reported of an application I made: . When I submit a form it sometimes fails and I get an empty form again. But only sometimes. . It took quite a while to find out what actually happened. What made it hard to debug (but finally gave me the answer) is that the posted information didn’t end up in the server. So either the browser or some middleware was messing it up. . Locally I never had any problems. It always worked. But the client was using confidential information in their tests. This turned out a red herring. . The solution was to look at the requests made by the client: whenever it went wrong, there was a 302 redirect to authenticate again. Turns out if a POST request results in a redirect, it will be a GET request, essentially all POST information gets lost See Stack Overflow. After authenticating Keycloak redirected back, to an empty form. For the client, who didn’t see the redirect, it was as if the server just lost their input. . For working with OIDC I noticed that the OIDC library only redirects if it cannot refresh the token via a direct HTTP call to the server. Additionally, I was using multiple workers for the Flask application without distributed storage of OIDC information. . So the solution was: . Let people be logged in longer (internal application, so that’s fine); | Create a simple storage to share information between workers, so refreshes go via an HTTP call not a redirect. | .",
            "url": "vanderzwaan.dev/2019/06/13/http-redirects-and-funny-behaviour.html",
            "relUrl": "/2019/06/13/http-redirects-and-funny-behaviour.html",
            "date": " • Jun 13, 2019"
        }
        
    
  
    
        ,"post22": {
            "title": "Decimals and DynamoDB",
            "content": "A short note on DynamoDB of AWS and how it suprisingly gives Decimals when you extract records. And forced me to write ducktape-code. . The story . DynamoDB implicitely converts integers to decimals. Decimals cause some problems in Python code since it is not a basic type. For example, json.dumps() doesn’t work anymore. . You put a Python datastructure with lists and dictionairies into DynamoDB with strings and integers. | DynamoDB converts and stores the datastructure | You load an entry from DynamoDB | …it suddenly contains Decimals instead of integers | …and json.dumps() crashes (among others) | The ‘solution’ . I now simply convert decimals back to float and int. The alternative of storing the object as a string (convert to JSON) I dislike. Figuring out how to configure DynamoDB…I couldn’t find a decent way within half an hour. I did find a discussion on a feature request to disable decimals and/or find some sane way to work with it, but no solutions. . So I wrote a little function to convert a dictionary: . def convert_decimal(dictionary): &quot;&quot;&quot; Converts decimals to float and int. &quot;&quot;&quot; for key in dictionary.keys(): value = dictionairy[key] if isinstance(value, Decimal): if v % 1 == 0: dictionary[key] = int(value) else: dictionary[key] = float(value) return dictionary .",
            "url": "vanderzwaan.dev/2019/05/02/decimals-and-dynamodb.html",
            "relUrl": "/2019/05/02/decimals-and-dynamodb.html",
            "date": " • May 2, 2019"
        }
        
    
  
    
        ,"post23": {
            "title": "Flask Pdf Download",
            "content": "This is a short note on getting PDF downloads working on Flask. . The goal is a button with ‘Download as PDF’: . Generate an HTML page | Convert his page to a PDF | Download the PDF while staying on the same page | . This seemed easy: just use pdfkit, get the correct dependencies and go. It turns out there were some hiccups for which I needed to combine several answers. . wkhtml did not work headless, I needed to patch it | PDFkit examples and the API ask for a filename, I did not want to store the PDF | PDFkit returns bytes and not a fileobject that Flask expects | For a few minutes I forgot about caching headers. I always want to generate a PDF on the current version | . Getting PDFkit to run on a Docker image . I used the following code to get it working. It uses apt-get to get wkhtmltopdf and its dependencies. Then, I patched wkhtmltopdf to make it work headless. . RUN apt-get update RUN apt-get install -y wkhtmltopdf RUN wget -nv https://downloads.wkhtmltopdf.org/0.12/0.12.5/wkhtmltox_0.12.5-1.stretch_amd64.deb RUN apt install -y ./wkhtmltox_0.12.5-1.stretch_amd64.deb RUN pip install pdfkit . For some reason (there are more people talking about this issue) the headless part does not work out-of-the-box due to some upstream problem of a dependency. . Generate the PDF . import pdfkit # ...Import other Flask stuff # Render the HTML print_html = render_template(&#39;print.html&#39;) # Convert the HTML to PDF, as target filename give &#39;False&#39; pdf = pdfkit.from_string(print_html, False) # Convert the bytes to a file(like) file = io.BytesIO(pdf) # Optional: add a timestamp to the generated file. created_on = current_time_local().strftime(&#39;%Y-%m-%d %H:%M:%S&#39;) filename = f&quot;Filename ({created_on})&quot; # Output to the browser return send_file(file, attachment_filename=filename, mimetype=&#39;application/pdf&#39;, # Give this argument to let the user stay on the current page as_attachment=True, # Set a low cache timeout cache_timeout=-1) .",
            "url": "vanderzwaan.dev/2019/04/24/flask-pdf-download.html",
            "relUrl": "/2019/04/24/flask-pdf-download.html",
            "date": " • Apr 24, 2019"
        }
        
    
  
    
        ,"post24": {
            "title": "Python Partial",
            "content": "This is a short note on the Python function partial. . Partials are very common in functional languages such as Haskell and Elm. . Functional language . Suppose you need a function to color a line, it would map the position on the line (ranging from 0 to 1) to a RGB color like so (for convenience I noted the tuple as RGB instead of (Int, Int, Int)): . color :: Position -&gt; RGB . But we do not want to define a function for, say, all gradients: . blue_green :: Position -&gt; RGB blue_green p = (0, p * 255, (1 - p) * 255) + (0, (1 - p) * 255, p * 255) . We can now define a function: . gradient :: RGB -&gt; RGB -&gt; Position -&gt; RGB gradient l r p = (1 - p) * l + p * r . However we’re in a bit of a pickle since our line coloring expects a function that maps positions to RGB. Functional languages allow is to simply only pass the first two arguments: . color_function = gradient (0, 255, 50) (50, 0, 100) . This yields a function that maps positions to RGB and the functionality does not need to know how it was constructed. . Python . Python has the partial function to do this. . # Target signature and behaviour: def color (position): return (R, G, B) # Gradient coloring function def gradient(p, left=(0, 0, 0), right=(255, 255, 255)): return tuple((1 - p) * left[i] + p * right[i] for i in range(3)) # Now get a blue-green coloring function blue_green = partial(gradient, left=(0, 0, 255), right=(0, 255, 0)) . Alternatively: . # Target signature and behaviour: def color (position): return (R, G, B) # Gradient coloring function def gradient(left, right, p): return tuple((1 - p) * left[i] + p * right[i] for i in range(3)) # Now get a blue-green coloring function blue_green = partial(gradient, (0, 0, 255), (0, 255, 0)) .",
            "url": "vanderzwaan.dev/2019/04/16/python-partial.html",
            "relUrl": "/2019/04/16/python-partial.html",
            "date": " • Apr 16, 2019"
        }
        
    
  
    
        ,"post25": {
            "title": "Starlette and Socketio",
            "content": "A short note on how to use Starlette together with Python-SocketIO. For a DIY project I want to run a single thread process to control a wake-up light (among other things). . Starlette . Starlette is basically an asynchronous version on Flask, which are both web frameworks. Personally I use Flask quite often and like the philosophy and resulting code. . Starlette seems to take inspiration from Flask and improves on speed. Additionaly there are some nice quality-of-life improvements (Websocktes, GraphQL). The asynchrounous part gives must easier in-process background tasks, for example sending an e-mail. . In their own words: . Starlette is a lightweight ASGI framework/toolkit, which is ideal for building high performance asyncio services. . SocketIO (and Python-SocketIO) . Socket.IO is a library on top of websockets . Python-SocketIO is a Python library that enables you to work with socket.io in Python. The name is pretty descriptive: . This projects implements Socket.IO clients and servers that can run standalone or integrated with a variety of Python web frameworks. . The problem . I wanted the following things: . Starlette with a REST API; | SocketIO (through Python-SocketIO) for realtime streams; | A background task that periodially streams the current information in the system. | . Additionally I wanted to have it in a single thread, and no heavy extra libraries. Such as distributed task queues (Celery) which then require other moving parts. . Part of the reason is that I have a single state in memory of some hardware components. It being a single-threaded async application makes it very easy to argue about updating the state and reading the state. . I ran into some issues when wanting to combine Starlette+Python-SocketIO+background tasks. Starlette runs on Uvicorn and the current way how Python-SocketIO hooks into the async loop was not working anymore. . After some trial and error I got it working: . import logging import asyncio import uvicorn from uvicorn.loops.uvloop import uvloop_setup from starlette.applications import Starlette from starlette.responses import JSONResponse import socketio # Set some basic logging logging.basicConfig( level=2, format=&quot;%(asctime)-15s %(levelname)-8s %(message)s&quot; ) # Create a basic app sio = socketio.AsyncServer(async_mode=&#39;asgi&#39;) star_app = Starlette(debug=True) app = socketio.ASGIApp(sio, star_app) @star_app.route(&#39;/&#39;) async def homepage(request): return JSONResponse({&#39;hello&#39;: &#39;world&#39;}) @sio.on(&#39;connect&#39;) async def connect(sid, environ): logging.info(f&quot;connect {sid}&quot;) @sio.on(&#39;message&#39;) async def message(sid, data): logging.info(f&quot;message {data}&quot;) # await device.set(data) @sio.on(&#39;disconnect&#39;) async def disconnect(sid): logging.info(f&#39;disconnect {sid}&#39;) # Set up the event loop async def start_background_tasks(): while True: logging.info(f&quot;Background tasks that ticks every 10s.&quot;) await sio.sleep(10.0) async def start_uvicorn(): uvicorn.run(app, host=&#39;0.0.0.0&#39;, port=8000) async def main(loop): bg_task = loop.create_task(start_background_tasks()) uv_task = loop.create_task(start_uvicorn()) await asyncio.wait([bg_task, uv_task]) if __name__ == &#39;__main__&#39;: uvloop_setup() loop = asyncio.get_event_loop() loop.run_until_complete(main(loop)) loop.close() . Perhaps there are other people running into the same issue and I can save them some time. .",
            "url": "vanderzwaan.dev/2019/04/13/starlette-and-socketio.html",
            "relUrl": "/2019/04/13/starlette-and-socketio.html",
            "date": " • Apr 13, 2019"
        }
        
    
  
    
        ,"post26": {
            "title": "Posture of cats: machine learning with fast.ai",
            "content": "During a spare hour I decided to see how fast I could prototype something with the fast.ai library. The library is impressive, I can’t say otherwise. Very quickly I got a working prototype. The result was a fairly decent model to recognize the posture of a cat. . Fast.ai library . Recently I’ve decided to take a look at fast.ai, their course and their library. The best way to do this is to try something and it’s very likely you’ll end up with a cat-related topic. With my desk oriented towards the spot my cat was chilling, I decided to take the library for a spin and recognize cat postures. . To make the post self-contained I’ll note the snippets of code needed. As you’ll see it’s actually very little code - and every line is clear on what it does. . Note: Most of the code was copied from the example notebook in the course of fast.ai. . from fastai.vision import * from fastai.metrics import error_rate . The postures . I picked the first four postures that came to mind: . Lounging | Curled up | Sitting | Walking | . . Getting the data . There are some pictures of my cat but not sufficient. He is very lazy and is predominantly curled up. In the course of fast.ai they discussed the option to just use Google searches to get a dataset quickly. I have done this before, but they had some nice helpers to really make it a smooth experience: . 1) Do a Google Image search yourself. For example, search for “cats curled up” . 2) Run a JavaScript script they supply and obtain a file with urls . urls = Array.from(document.querySelectorAll(&#39;.rg_di .rg_meta&#39;)).map(el=&gt;JSON.parse(el.textContent).ou); window.open(&#39;data:text/csv;charset=utf-8,&#39; + escape(urls.join(&#39; n&#39;))); . 3) Run a function from their library to download and verify the images . download_images(&#39;path/urls_cats.csv&#39;, &#39;path/where/to/download/images&#39;, max_pics=200) verify_images(&#39;path/where/to/download/images&#39;, delete=True, max_size=500) . 4) Create a dataset . np.random.seed(42) # Set a fixed seed to always get the same train-validation split data = ImageDataBunch.from_folder(&quot;path/to/images&quot;, train=&quot;.&quot;, valid_pct=0.2, ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats) . Training the neural network . Training the neural network is actually very simple . 5) Create the neural network: . learn = cnn_learner(data, models.resnet50, metrics=error_rate) . 6) Find a a good learning rate: . learn.lr_find(start_lr=1e-4) learn.recorder.plot() . 7) Train the neural network: . learn.fit_one_cycle(12, max_lr=2e-2) . There are some additional steps you can take to further finetune the model, but notice how easy it is? . Performance . The performance was actually quite decent: an error rate of 17%. Especially if you realize that the the distinction between classes is fuzzy and can be subtle. The images that the google search returned quite often confused lounging with curled up and sitting with lounging. . Even I’m not totally clear on the distinction between lounging and curled up in some cases :) . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . . Let’s plot the images the model was the most confused about, and highlight the areas that the model was focused on: . interp.plot_top_losses(9, figsize=(15,11)) . . Improvements . As you can see in the top losses, the samples contain some images that are no good: multiple cats or to far zoomed in. The easiest thing to do would be to get a better dataset and make a clear distinction between the classes. . How does it work . The cool thing fast.ai did in their course is to encode as many best practices in the library as they can. This means that you don’t start with a neural network from scratch: you take a network architecture that is tested in practices, is pretrained and you get sensible defaults. . In my case I used ResNet50 and then fine-tuned it with the cat data. This means you’re getting a lot for free! .",
            "url": "vanderzwaan.dev/2019/04/10/cature.html",
            "relUrl": "/2019/04/10/cature.html",
            "date": " • Apr 10, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "You’ve clicked ‘About’ and want to know more about me. Probably you Googled, checked my LinkedIn and looked at Datasprong and MavenBlue. Down the rabbit hole to see my publications on Google Scholar. . Some infrequently asked questions: . Q: Do you have TikTok? . A: No. . | Q: What is your Erdős number? . A: 3 . | Q: What is your latest hobby project? . A: Self-made worm compost bin. . | . Feel free to contact me directly with your questions or remarks. Happy to talk! .",
          "url": "vanderzwaan.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "vanderzwaan.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}